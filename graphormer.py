# -*- coding: utf-8 -*-
"""Graphormer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EBehx8pOhn_cmS7SvMLKY5iWdPFWrDtV

GRAPHORMER MODEL FOR SMELL PREDICTION
"""

!pip install --pre deepchem
!pip install rdkit-pypi
from rdkit import rdBase
from rdkit import RDConfig
!pip install --pre deepchem
!pip install rdkit-pypi
!pip install rdkit-pypi
!pip install mordred
!pip install --pre deepchem
!pip install rdkit-pypi
import deepchem as dc
dc.__version__

!pip install torch_geometric

!pip install torch

import pandas as pd
import numpy as np
from google.colab import drive
drive.mount('/content/drive')
dataset = pd.read_csv('/content/drive/MyDrive/Resources for AI-23/sr/bs5/training_data.csv')
dataset1 = pd.read_csv('/content/drive/MyDrive/Resources for AI-23/sr/bs5/testing_data.csv')

print(dataset.shape)

print(dataset1.shape)

dataset.head()

"""Training data preprocessing"""

import pandas as pd

# Read the CSV file
df = pd.read_csv('/content/drive/MyDrive/Resources for AI-23/sr/bs5/training_data.csv')

# Remove rows with NaN values
df_cleaned_train = df.dropna()

# Print the cleaned DataFrame
print(df_cleaned_train)

import pandas as pd
# Convert the DataFrame to CSV format
df_cleaned_train.to_csv('cleaned_data_train.csv', index=False)

import pandas as pd
from rdkit import Chem

# Read the CSV file containing the data
df = pd.read_csv('cleaned_data_train.csv')

# Remove invalid SMILES
valid_indices = []
for i, smiles in enumerate(df['SMILES']):
    mol = Chem.MolFromSmiles(smiles)
    if mol is not None:
        valid_indices.append(i)

# Filter the DataFrame using the valid indices
df_cleaned = df.iloc[valid_indices]

# Save the cleaned DataFrame to a new CSV file
df_cleaned.to_csv('cleaned_train.csv', index=False)

print(df_cleaned.shape)

"""Test data preprocessing"""

import pandas as pd

# Read the CSV file
df = pd.read_csv('/content/drive/MyDrive/Resources for AI-23/sr/bs5/testing_data.csv')

# Remove rows with NaN values
df_cleaned_train = df.dropna()

# Print the cleaned DataFrame
print(df_cleaned_train)

import pandas as pd

# Assuming you have a cleaned DataFrame named df_cleaned
# Convert the DataFrame to CSV format
df_cleaned_train.to_csv('cleaned_data_test.csv', index=False)

import pandas as pd
from rdkit import Chem

# Read the CSV file containing the data
df = pd.read_csv('cleaned_data_test.csv')

# Remove invalid SMILES
valid_indices = []
for i, smiles in enumerate(df['SMILES']):
    mol = Chem.MolFromSmiles(smiles)
    if mol is not None:
        valid_indices.append(i)

# Filter the DataFrame using the valid indices
df_cleaned = df.iloc[valid_indices]

# Save the cleaned DataFrame to a new CSV file
df_cleaned.to_csv('cleaned_test.csv', index=False)

print(df_cleaned.shape)

"""a.) ATOMIC REPRESENTATIONS"""

import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem

# Read the DataFrame containing the  dataset
df_cleaned_train = pd.read_csv('cleaned_train.csv')

# Create a new column to store the atomic representations
df_cleaned_train['Atomic_Representations'] = None

# Iterate over the SMILES strings and generate atomic representations
for i, smiles in enumerate(df_cleaned_train['SMILES']):
    mol = Chem.MolFromSmiles(smiles)  # Convert SMILES to RDKit molecule object
    if mol is not None:
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2)  # Generate Morgan fingerprint
        df_cleaned_train.at[i, 'Atomic_Representations'] = fp

# Save the updated DataFrame with atomic representations
df_cleaned_train.to_csv('stat_dataset_train_with_atomic.csv', index=False)

import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem import AllChem

# Read the DataFrame containing the  dataset
df_cleaned_train = pd.read_csv('cleaned_train.csv')

# Create an empty list to store the feature vectors
feature_vectors = []

# Iterate over the SMILES strings and generate atomic representations
for smiles in df_cleaned_train['SMILES']:
    mol = Chem.MolFromSmiles(smiles)  # Convert SMILES to RDKit molecule object
    if mol is not None:
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2)  # Generate Morgan fingerprint
        feature_vector = np.array(list(fp), dtype=int)  # Convert fingerprint to numpy array
        feature_vectors.append(feature_vector)

# Convert the list of feature vectors to a numpy array
feature_matrix = np.array(feature_vectors)

# Save the feature matrix to a file
np.save('feature_matrix_train.npy', feature_matrix)

# Load the feature matrix from the file
feature_matrix_train = np.load('feature_matrix_train.npy')

# Get the shape of the feature matrix
num_samples, num_features = feature_matrix_train.shape
print(f"Number of samples: {num_samples}")
print(f"Number of features: {num_features}")

# Access a specific feature vector
sample_index = 0
feature_vector = feature_matrix_train[sample_index]
print(f"Feature vector for sample {sample_index}: {feature_vector}")

# Load the feature matrix from the file
feature_matrix_train = np.load('feature_matrix_train.npy')

# Iterate over the feature matrix and print each feature vector
for i, feature_vector in enumerate(feature_matrix_train):
    print(f"Feature vector for sample {i}: {feature_vector}")

print(feature_matrix_train.shape)

import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem


df_cleaned_train = pd.read_csv('cleaned_test.csv')

# Create a new column to store the atomic representations
df_cleaned_train['Atomic_Representations'] = None

# Iterate over the SMILES strings and generate atomic representations
for i, smiles in enumerate(df_cleaned_train['SMILES']):
    mol = Chem.MolFromSmiles(smiles)  # Convert SMILES to RDKit molecule object
    if mol is not None:
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2)  # Generate Morgan fingerprint
        df_cleaned_train.at[i, 'Atomic_Representations'] = fp

# Save the updated DataFrame with atomic representations
df_cleaned_train.to_csv('stat_dataset_test_with_atomic.csv', index=False)

import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem import AllChem

# Read the DataFrame containing the dataset
df_cleaned_train = pd.read_csv('cleaned_test.csv')

# Create an empty list to store the feature vectors
feature_vectors = []

# Iterate over the SMILES strings and generate atomic representations
for smiles in df_cleaned_train['SMILES']:
    mol = Chem.MolFromSmiles(smiles)  # Convert SMILES to RDKit molecule object
    if mol is not None:
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2)  # Generate Morgan fingerprint
        feature_vector = np.array(list(fp), dtype=int)  # Convert fingerprint to numpy array
        feature_vectors.append(feature_vector)

# Convert the list of feature vectors to a numpy array
feature_matrix = np.array(feature_vectors)

# Save the feature matrix to a file
np.save('feature_matrix_test.npy', feature_matrix)

# Load the feature matrix from the file (optional if you have not restarted the Python session)
feature_matrix_test = np.load('feature_matrix_test.npy')

# Get the shape of the feature matrix
num_samples, num_features = feature_matrix.shape
print(f"Number of samples: {num_samples}")
print(f"Number of features: {num_features}")

# Access a specific feature vector
sample_index = 0
feature_vector = feature_matrix[sample_index]
print(f"Feature vector for sample {sample_index}: {feature_vector}")

# Get the shape of the feature matrix
num_samples, num_features = feature_matrix_test.shape
print(f"Number of samples: {num_samples}")
print(f"Number of features: {num_features}")
import numpy as np

# Load the feature matrix from the file
feature_matrix_test = np.load('feature_matrix_test.npy')

# Get the shape of the feature matrix
num_samples, num_features = feature_matrix_test.shape
print(f"Number of samples: {num_samples}")
print(f"Number of features: {num_features}")

# Access a specific feature vector
sample_index = 0
feature_vector = feature_matrix_test[sample_index]
print(f"Feature vector for sample {sample_index}: {feature_vector}")

feature_matrix_test = np.load('feature_matrix_test.npy')

# Iterate over the feature matrix and print each feature vector
for i, feature_vector in enumerate(feature_matrix):
    print(f"Feature vector for sample {i}: {feature_vector}")

print(feature_matrix_test.shape)

"""GRAPH METHODS

1. GAT
"""

!pip install torch_geometric
!pip install torch

!pip install matplotlib

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GAT model
class GAT(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1):
        super(GAT, self).__init__()
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, output_dim, heads=1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GAT model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2

# Create an instance of the GAT model
model = GAT(input_dim, hidden_dim, output_dim, num_heads)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER - GAT"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
from torch.nn import TransformerEncoder, TransformerEncoderLayer
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as edge_index tensors
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i] for i in range(num_nodes_train)], dtype=torch.long)

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i] for i in range(num_nodes_test)], dtype=torch.long)

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test)

# Define the GAT-Transformer hybrid model
class GATTransformer(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1, num_layers=1):
        super(GATTransformer, self).__init__()

        # GAT component
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1)

        # Transformer component
        self.embedding = nn.Linear(input_dim, hidden_dim)
        encoder_layers = TransformerEncoderLayer(hidden_dim, num_heads)
        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)

        # Output layer
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        # GAT component
        x_gat = self.embedding(x)
        x_gat = F.relu(self.conv1(x_gat, edge_index))
        x_gat = self.conv2(x_gat, edge_index)

        # Transformer component
        x_trans = x_gat.permute(1, 0, 2)  # Adjust dimensions for transformer input
        x_trans = self.transformer_encoder(x_trans)
        x_trans = x_trans.permute(1, 0, 2)  # Adjust dimensions back to original

        # Output layer
        x_out = self.fc(x_trans[:, -1, :])  # Take the last hidden state and pass it through a fully connected layer
        return x_out

# Define the dimensions of the GAT-Transformer model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2
num_layers = 2

# Create an instance of the GAT-Transformer model
model = GATTransformer(input_dim, hidden_dim, output_dim, num_heads, num_layers)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)

    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)

    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()

    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)

    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

"""2. GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Import the required modules for the Transformer
import torch.nn.Transformer as nnTransformer
import torch.nn.TransformerEncoder as nnTransformerEncoder
import torch.nn.TransformerEncoderLayer as nnTransformerEncoderLayer

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model with Transformers
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nnTransformerEncoderLayer(input_dim, hidden_dim),
            nn.ReLU(),
            nnTransformerEncoderLayer(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nnTransformerEncoderLayer(hidden_dim, hidden_dim),
            nn.ReLU(),
            nnTransformerEncoderLayer(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

"""GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv

# Load the feature matrix from the file
feature_matrix_test = np.load('feature_matrix_test.npy')

# Define the adjacency matrix (graph structure)
adjacency_matrix = np.eye(feature_matrix_test.shape[0])  # Assuming an identity adjacency matrix

# Convert the feature matrix and adjacency matrix to PyTorch tensors
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
adjacency_matrix = torch.from_numpy(adjacency_matrix).float()

# Create the edge_index tensor
num_nodes = feature_matrix_test.shape[0]
edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()

# Create a PyTorch Geometric data object
data = Data(x=feature_matrix_test, edge_index=edge_index, edge_attr=adjacency_matrix)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_test.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Perform a forward pass to obtain the node embeddings
node_embeddings = model(data.x, data.edge_index)

# Print the shape of the node embeddings
#print(f"Node embeddings shape: {node_embeddings.shape}")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Import the required modules for the Transformer
import torch.nn.Transformer as nnTransformer
import torch.nn.TransformerEncoder as nnTransformerEncoder
import torch.nn.TransformerEncoderLayer as nnTransformerEncoderLayer

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model with Transformers
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()

        self.transformer_encoder = nnTransformerEncoder(
            nnTransformerEncoderLayer(input_dim, hidden_dim),
            num_layers=2
        )
        self.linear = nn.Linear(input_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.transformer_encoder(x)
        x = F.relu(x)
        x = self.linear(x)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

"""b.) SUBSTRING REPRESENATATIONS"""

import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Substring representation
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_train = np.array(substring_features_train)

# Print the shape of the substring features
print("Substring features shape:", substring_features_train.shape)
np.save('feature_matrix_train.npy', substring_features_train)

import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Substring representation
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_test = np.array(substring_features_train)

# Print the shape of the substring features
print("Substring features shape:", substring_features_test.shape)
# Save the substring features to a .npy file
np.save('feature_matrix_test.npy', substring_features_test)

# Perform further preprocessing or use the substring features for model training

"""GRAPH METHODS

1. GAT
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GAT model
class GAT(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1):
        super(GAT, self).__init__()
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, output_dim, heads=1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GAT model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2

# Create an instance of the GAT model
model = GAT(input_dim, hidden_dim, output_dim, num_heads)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GAT"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
from torch.nn import TransformerEncoder, TransformerEncoderLayer
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as edge_index tensors
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i] for i in range(num_nodes_train)], dtype=torch.long)

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i] for i in range(num_nodes_test)], dtype=torch.long)

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test)

# Define the GAT-Transformer hybrid model
class GATTransformer(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1, num_layers=1):
        super(GATTransformer, self).__init__()

        # GAT component
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1)

        # Transformer component
        self.embedding = nn.Linear(input_dim, hidden_dim)
        encoder_layers = TransformerEncoderLayer(hidden_dim, num_heads)
        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)

        # Output layer
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        # GAT component
        x_gat = self.embedding(x)
        x_gat = F.relu(self.conv1(x_gat, edge_index))
        x_gat = self.conv2(x_gat, edge_index)

        # Transformer component
        x_trans = x_gat.permute(1, 0, 2)  # Adjust dimensions for transformer input
        x_trans = self.transformer_encoder(x_trans)
        x_trans = x_trans.permute(1, 0, 2)  # Adjust dimensions back to original

        # Output layer
        x_out = self.fc(x_trans[:, -1, :])  # Take the last hidden state and pass it through a fully connected layer
        return x_out

# Define the dimensions of the GAT-Transformer model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2
num_layers = 2

# Create an instance of the GAT-Transformer model
model = GATTransformer(input_dim, hidden_dim, output_dim, num_heads, num_layers)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)

    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)

    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()

    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)

    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)



"""2. GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GIN"""

import torch
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Import the required modules for the Transformer
import torch.nn.Transformer as nnTransformer
import torch.nn.TransformerEncoder as nnTransformerEncoder
import torch.nn.TransformerEncoderLayer as nnTransformerEncoderLayer

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model with Transformers
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nnTransformerEncoderLayer(input_dim, hidden_dim),
            nn.ReLU(),
            nnTransformerEncoderLayer(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nnTransformerEncoderLayer(hidden_dim, hidden_dim),
            nn.ReLU(),
            nnTransformerEncoderLayer(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

"""3. GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv

# Load the feature matrix from the file
feature_matrix_test = np.load('feature_matrix_test.npy')

# Define the adjacency matrix (graph structure)
adjacency_matrix = np.eye(feature_matrix_test.shape[0])  # Assuming an identity adjacency matrix

# Convert the feature matrix and adjacency matrix to PyTorch tensors
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
adjacency_matrix = torch.from_numpy(adjacency_matrix).float()

# Create the edge_index tensor
num_nodes = feature_matrix_test.shape[0]
edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()

# Create a PyTorch Geometric data object
data = Data(x=feature_matrix_test, edge_index=edge_index, edge_attr=adjacency_matrix)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_test.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Perform a forward pass to obtain the node embeddings
node_embeddings = model(data.x, data.edge_index)

# Print the shape of the node embeddings
#print(f"Node embeddings shape: {node_embeddings.shape}")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAHORMER GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Import the required modules for the Transformer
import torch.nn.Transformer as nnTransformer
import torch.nn.TransformerEncoder as nnTransformerEncoder
import torch.nn.TransformerEncoderLayer as nnTransformerEncoderLayer

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model with Transformers
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()

        self.transformer_encoder = nnTransformerEncoder(
            nnTransformerEncoderLayer(input_dim, hidden_dim),
            num_layers=2
        )
        self.linear = nn.Linear(input_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.transformer_encoder(x)
        x = F.relu(x)
        x = self.linear(x)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

"""## c.) MOLECULAR GRAPHS"""

from rdkit import Chem
from rdkit.Chem import Draw
import pandas as pd

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
smiles_train = df_train['SMILES'].values

# Generate RDKit molecules from SMILES strings
mols_train = [Chem.MolFromSmiles(smiles) for smiles in smiles_train]

# Generate molecular images
images = [Draw.MolToImage(mol) for mol in mols_train]

# Display the molecular images
for image in images:
    image.show()

from rdkit import Chem
from rdkit.Chem import Draw
import pandas as pd

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')
smiles_train = df_train['SMILES'].values

# Generate RDKit molecules from SMILES strings
mols_train = [Chem.MolFromSmiles(smiles) for smiles in smiles_train]

# Generate molecular images
images = [Draw.MolToImage(mol) for mol in mols_train]

# Display the molecular images
for image in images:
    image.show()

"""Molecular Represenation where we use RDKit to convert the SMILES to molecular objects and generate molecular images, which are saved as separate image files.

Next, we calculate several molecular descriptors for each molecule and generates a molecular fingerprint using the Morgan fingerprint algorithm. The descriptors and fingerprint are combined into a feature vector, and the feature vectors are collected to form a feature matrix.
"""

import pandas as pd
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem

# Read the SMILES from the cleaned_train.csv file
data = pd.read_csv('cleaned_train.csv')
smiles = data['SMILES']

# Generate molecular image representation
mols = [Chem.MolFromSmiles(smile) for smile in smiles]
images = [Draw.MolToImage(mol) for mol in mols]
for i, image in enumerate(images):
    image.save(f'mol_{i}.png')  # Save the molecular images

# Generate feature matrix
feature_matrix = []
for mol in mols:
    # Calculate molecular descriptors
    descriptors = [Descriptors.MolLogP(mol),
                   Descriptors.MolMR(mol),
                   Descriptors.TPSA(mol),
                   Descriptors.NumHDonors(mol),
                   Descriptors.NumHAcceptors(mol)]

    # Calculate molecular fingerprint (Morgan fingerprint)
    fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2043)

    # Convert fingerprint to a numpy array
    fingerprint_array = []
    for i in range(fingerprint.GetNumBits()):
        bit = fingerprint.GetBit(i)
        fingerprint_array.append(bit)

    # Combine descriptors and fingerprint into a feature vector
    feature_vector = descriptors + fingerprint_array
    feature_matrix.append(feature_vector)

# Convert the feature matrix to a pandas DataFrame
columns = ['LogP', 'MR', 'TPSA', 'NumHDonors', 'NumHAcceptors'] + \
          [f'Bit_{i}' for i in range(2043)]
feature_df = pd.DataFrame(feature_matrix, columns=columns)

# Save the feature matrix to a CSV file
feature_df.to_csv('feature_matrix_train.csv', index=False)

import pandas as pd
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem

# Read the SMILES from the cleaned_train.csv file
data = pd.read_csv('cleaned_test.csv')
smiles = data['SMILES']

# Generate molecular image representation
mols = [Chem.MolFromSmiles(smile) for smile in smiles]
images = [Draw.MolToImage(mol) for mol in mols]
for i, image in enumerate(images):
    image.save(f'mol_{i}.png')  # Save the molecular images

# Generate feature matrix
feature_matrix = []
for mol in mols:
    # Calculate molecular descriptors
    descriptors = [Descriptors.MolLogP(mol),
                   Descriptors.MolMR(mol),
                   Descriptors.TPSA(mol),
                   Descriptors.NumHDonors(mol),
                   Descriptors.NumHAcceptors(mol)]

    # Calculate molecular fingerprint (Morgan fingerprint)
    fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2043)

    # Convert fingerprint to a numpy array
    fingerprint_array = []
    for i in range(fingerprint.GetNumBits()):
        bit = fingerprint.GetBit(i)
        fingerprint_array.append(bit)

    # Combine descriptors and fingerprint into a feature vector
    feature_vector = descriptors + fingerprint_array
    feature_matrix.append(feature_vector)

# Convert the feature matrix to a pandas DataFrame
columns = ['LogP', 'MR', 'TPSA', 'NumHDonors', 'NumHAcceptors'] + \
          [f'Bit_{i}' for i in range(2043)]
feature_df = pd.DataFrame(feature_matrix, columns=columns)

# Save the feature matrix to a CSV file
feature_df.to_csv('feature_matrix_test.csv', index=False)

print(feature_df.shape)

"""3.2 GraphSAGE"""

!pip install networkx
!pip install igraph
!pip install spektral

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_train.npy', adjacency_matrix_train)

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_test.npy', adjacency_matrix_train)

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Reshape
from spektral.layers import GraphSageConv
from spektral.datasets import citation
from spektral.utils import normalized_adjacency
from scipy.sparse import coo_matrix
from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the adjacency matrix
adjacency_matrix_train = np.load('adjacency_matrix_train.npy')
adjacency_matrix_test = np.load('adjacency_matrix_test.npy')

# Convert adjacency matrix to SparseTensor
adjacency_matrix_train = coo_matrix(adjacency_matrix_train)
indices = np.column_stack((adjacency_matrix_train.row, adjacency_matrix_train.col))
adjacency_matrix_train = tf.sparse.SparseTensor(indices, adjacency_matrix_train.data, adjacency_matrix_train.shape)

adjacency_matrix_test = coo_matrix(adjacency_matrix_test)
indices = np.column_stack((adjacency_matrix_test.row, adjacency_matrix_test.col))
adjacency_matrix_test = tf.sparse.SparseTensor(indices, adjacency_matrix_test.data, adjacency_matrix_test.shape)

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features,)

# Define the GraphSAGE model architecture
x_in = tf.keras.Input(shape=input_shape)
a_in = tf.keras.Input(shape=(None,), sparse=True)

x = GraphSageConv(64, activation='relu')([x_in, a_in])
x = Dropout(0.5)(x)
x = GraphSageConv(64, activation='relu')([x, a_in])
x = Dropout(0.5)(x)
x = tf.keras.layers.Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(units=1, activation='sigmoid')(x)

# Create the model
model = Model(inputs=[x_in, a_in], outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x=[feature_matrix_train, adjacency_matrix_train], y=labels_train, batch_size=32, epochs=2000, validation_data=([feature_matrix_test, adjacency_matrix_test], labels_test))

# Evaluate the model
y_pred = model.predict([feature_matrix_test, adjacency_matrix_test])
y_pred_binary = (y_pred >= 0.5).astype(int)
print("Classification Report:")
print(classification_report(labels_test, y_pred_binary))

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(labels_test, y_pred)
aupr = np.trapz(precision, recall)
#print(f"AUPR: {aupr}")

# Calculate AUROC
auroc = roc_auc_score(labels_test, y_pred)
print(f"AUROC: {auroc}")

# Calculate specificity

"""GRAPH METHODS

1.GAT
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GAT model
class GAT(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1):
        super(GAT, self).__init__()
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, output_dim, heads=1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GAT model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2

# Create an instance of the GAT model
model = GAT(input_dim, hidden_dim, output_dim, num_heads)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GAT"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
from torch.nn import TransformerEncoder, TransformerEncoderLayer
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as edge_index tensors
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i] for i in range(num_nodes_train)], dtype=torch.long)

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i] for i in range(num_nodes_test)], dtype=torch.long)

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test)

# Define the GAT-Transformer hybrid model
class GATTransformer(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1, num_layers=1):
        super(GATTransformer, self).__init__()

        # GAT component
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1)

        # Transformer component
        self.embedding = nn.Linear(input_dim, hidden_dim)
        encoder_layers = TransformerEncoderLayer(hidden_dim, num_heads)
        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)

        # Output layer
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        # GAT component
        x_gat = self.embedding(x)
        x_gat = F.relu(self.conv1(x_gat, edge_index))
        x_gat = self.conv2(x_gat, edge_index)

        # Transformer component
        x_trans = x_gat.permute(1, 0, 2)  # Adjust dimensions for transformer input
        x_trans = self.transformer_encoder(x_trans)
        x_trans = x_trans.permute(1, 0, 2)  # Adjust dimensions back to original

        # Output layer
        x_out = self.fc(x_trans[:, -1, :])  # Take the last hidden state and pass it through a fully connected layer
        return x_out

# Define the dimensions of the GAT-Transformer model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2
num_layers = 2

# Create an instance of the GAT-Transformer model
model = GATTransformer(input_dim, hidden_dim, output_dim, num_heads, num_layers)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)

    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)

    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()

    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)

    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

"""2.GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Import the required modules for the Transformer
import torch.nn.Transformer as nnTransformer
import torch.nn.TransformerEncoder as nnTransformerEncoder
import torch.nn.TransformerEncoderLayer as nnTransformerEncoderLayer

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model with Transformers
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nnTransformerEncoderLayer(input_dim, hidden_dim),
            nn.ReLU(),
            nnTransformerEncoderLayer(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nnTransformerEncoderLayer(hidden_dim, hidden_dim),
            nn.ReLU(),
            nnTransformerEncoderLayer(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

"""3.GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv

# Load the feature matrix from the file
feature_matrix_test = np.load('feature_matrix_test.npy')

# Define the adjacency matrix (graph structure)
adjacency_matrix = np.eye(feature_matrix_test.shape[0])  # Assuming an identity adjacency matrix

# Convert the feature matrix and adjacency matrix to PyTorch tensors
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
adjacency_matrix = torch.from_numpy(adjacency_matrix).float()

# Create the edge_index tensor
num_nodes = feature_matrix_test.shape[0]
edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()

# Create a PyTorch Geometric data object
data = Data(x=feature_matrix_test, edge_index=edge_index, edge_attr=adjacency_matrix)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_test.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Perform a forward pass to obtain the node embeddings
node_embeddings = model(data.x, data.edge_index)

# Print the shape of the node embeddings
#print(f"Node embeddings shape: {node_embeddings.shape}")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Import the required modules for the Transformer
import torch.nn.Transformer as nnTransformer
import torch.nn.TransformerEncoder as nnTransformerEncoder
import torch.nn.TransformerEncoderLayer as nnTransformerEncoderLayer

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model with Transformers
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()

        self.transformer_encoder = nnTransformerEncoder(
            nnTransformerEncoderLayer(input_dim, hidden_dim),
            num_layers=2
        )
        self.linear = nn.Linear(input_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.transformer_encoder(x)
        x = F.relu(x)
        x = self.linear(x)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

"""# COMBINED DESCRIPTORS AND FEATURIZATION - TWO LEVEL AND MULTILEVEL GRANULARITY

4. TWO LEVEL GRANULARITY - AC+SS
"""

#training data
import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Atomic representation
atomic_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    atomic_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)  # Modify parameters as needed
    atomic_features_train.append(atomic_features)

atomic_features_train = np.array(atomic_features_train)

# Substring representation (using n-grams)
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_train = np.array(substring_features_train)

# Generate feature matrix
feature_matrix_train = np.concatenate((atomic_features_train, substring_features_train), axis=1)

# Print the shape of the feature matrix
print("Feature matrix shape:", feature_matrix_train.shape)

# Save the substring features to a .npy file
np.save('features_matrix_train.npy', feature_matrix_train)

#testing data
import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Atomic representation
atomic_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    atomic_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)  # Modify parameters as needed
    atomic_features_train.append(atomic_features)

atomic_features_train = np.array(atomic_features_train)

# Substring representation (using n-grams)
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_train = np.array(substring_features_train)

# Generate feature matrix
feature_matrix_train = np.concatenate((atomic_features_train, substring_features_train), axis=1)

# Print the shape of the feature matrix
print("Feature matrix shape:", feature_matrix_train.shape)

# Save the substring features to a .npy file
np.save('features_matrix_test.npy', feature_matrix_train)

"""4.4 GraphSAGE"""

!pip install networkx
!pip install igraph
!pip install spektral

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_train.npy', adjacency_matrix_train)

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_test.npy', adjacency_matrix_train)

"""EARLY STOPPING GRAPHSAGE"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Reshape
from spektral.layers import GraphSageConv
from spektral.datasets import citation
from spektral.utils import normalized_adjacency
from scipy.sparse import coo_matrix
from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the adjacency matrix
adjacency_matrix_train = np.load('adjacency_matrix_train.npy')
adjacency_matrix_test = np.load('adjacency_matrix_test.npy')

# Convert adjacency matrix to SparseTensor
adjacency_matrix_train = coo_matrix(adjacency_matrix_train)
indices = np.column_stack((adjacency_matrix_train.row, adjacency_matrix_train.col))
adjacency_matrix_train = tf.sparse.SparseTensor(indices, adjacency_matrix_train.data, adjacency_matrix_train.shape)

adjacency_matrix_test = coo_matrix(adjacency_matrix_test)
indices = np.column_stack((adjacency_matrix_test.row, adjacency_matrix_test.col))
adjacency_matrix_test = tf.sparse.SparseTensor(indices, adjacency_matrix_test.data, adjacency_matrix_test.shape)

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features,)

# Define the GraphSAGE model architecture
x_in = tf.keras.Input(shape=input_shape)
a_in = tf.keras.Input(shape=(None,), sparse=True)

x = GraphSageConv(64, activation='relu')([x_in, a_in])
x = Dropout(0.5)(x)
x = GraphSageConv(64, activation='relu')([x, a_in])
x = Dropout(0.5)(x)
x = tf.keras.layers.Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(units=1, activation='sigmoid')(x)

# Create the model
model = Model(inputs=[x_in, a_in], outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(x=[feature_matrix_train, adjacency_matrix_train], y=labels_train, batch_size=32, epochs=2000,
          validation_data=([feature_matrix_test, adjacency_matrix_test], labels_test),
          callbacks=[early_stopping])

# Evaluate the model
y_pred = model.predict([feature_matrix_test, adjacency_matrix_test])
y_pred_binary = (y_pred >= 0.5).astype(int)
print("Classification Report:")
print(classification_report(labels_test, y_pred_binary))

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(labels_test, y_pred)
aupr = np.trapz(precision, recall)
#print(f"AUPR: {aupr}")

# Calculate AUROC
auroc = roc_auc_score(labels_test, y_pred)
print(f"AUROC: {auroc}")

"""4.5 GAT"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GAT model
class GAT(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1):
        super(GAT, self).__init__()
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, output_dim, heads=1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GAT model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2

# Create an instance of the GAT model
model = GAT(input_dim, hidden_dim, output_dim, num_heads)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GAT"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
from torch.nn import TransformerEncoder, TransformerEncoderLayer
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as edge_index tensors
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i] for i in range(num_nodes_train)], dtype=torch.long)

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i] for i in range(num_nodes_test)], dtype=torch.long)

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test)

# Define the GAT-Transformer hybrid model
class GATTransformer(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1, num_layers=1):
        super(GATTransformer, self).__init__()

        # GAT component
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1)

        # Transformer component
        self.embedding = nn.Linear(input_dim, hidden_dim)
        encoder_layers = TransformerEncoderLayer(hidden_dim, num_heads)
        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)

        # Output layer
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        # GAT component
        x_gat = self.embedding(x)
        x_gat = F.relu(self.conv1(x_gat, edge_index))
        x_gat = self.conv2(x_gat, edge_index)

        # Transformer component
        x_trans = x_gat.permute(1, 0, 2)  # Adjust dimensions for transformer input
        x_trans = self.transformer_encoder(x_trans)
        x_trans = x_trans.permute(1, 0, 2)  # Adjust dimensions back to original

        # Output layer
        x_out = self.fc(x_trans[:, -1, :])  # Take the last hidden state and pass it through a fully connected layer
        return x_out

# Define the dimensions of the GAT-Transformer model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2
num_layers = 2

# Create an instance of the GAT-Transformer model
model = GATTransformer(input_dim, hidden_dim, output_dim, num_heads, num_layers)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)

    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)

    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()

    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)

    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

"""GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Import the required modules for the Transformer
import torch.nn.Transformer as nnTransformer
import torch.nn.TransformerEncoder as nnTransformerEncoder
import torch.nn.TransformerEncoderLayer as nnTransformerEncoderLayer

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model with Transformers
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nnTransformerEncoderLayer(input_dim, hidden_dim),
            nn.ReLU(),
            nnTransformerEncoderLayer(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nnTransformerEncoderLayer(hidden_dim, hidden_dim),
            nn.ReLU(),
            nnTransformerEncoderLayer(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

"""GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv

# Load the feature matrix from the file
feature_matrix_test = np.load('feature_matrix_test.npy')

# Define the adjacency matrix (graph structure)
adjacency_matrix = np.eye(feature_matrix_test.shape[0])  # Assuming an identity adjacency matrix

# Convert the feature matrix and adjacency matrix to PyTorch tensors
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
adjacency_matrix = torch.from_numpy(adjacency_matrix).float()

# Create the edge_index tensor
num_nodes = feature_matrix_test.shape[0]
edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()

# Create a PyTorch Geometric data object
data = Data(x=feature_matrix_test, edge_index=edge_index, edge_attr=adjacency_matrix)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_test.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Perform a forward pass to obtain the node embeddings
node_embeddings = model(data.x, data.edge_index)

# Print the shape of the node embeddings
#print(f"Node embeddings shape: {node_embeddings.shape}")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Import the required modules for the Transformer
import torch.nn.Transformer as nnTransformer
import torch.nn.TransformerEncoder as nnTransformerEncoder
import torch.nn.TransformerEncoderLayer as nnTransformerEncoderLayer

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model with Transformers
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()

        self.transformer_encoder = nnTransformerEncoder(
            nnTransformerEncoderLayer(input_dim, hidden_dim),
            num_layers=2
        )
        self.linear = nn.Linear(input_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.transformer_encoder(x)
        x = F.relu(x)
        x = self.linear(x)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

"""5. TWO LEVEL GRANULARITY - SS+IMG"""

import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Molecular representation
mol_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048)  # Modify parameters as needed
    mol_features_train.append(mol_features)

mol_features_train = np.array(mol_features_train)

# Print the shape of the molecular features
print("Molecular features shape:", mol_features_train.shape)

# Substring representation
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_train = np.array(substring_features_train)

# Print the shape of the substring features
print("Substring features shape:", substring_features_train.shape)

# Save the feature matrices
np.save('feature_matrix_train_molecular.npy', mol_features_train)
np.save('feature_matrix_train_substring.npy', substring_features_train)
import numpy as np

# Load the molecular and substring feature matrices
mol_features_train = np.load('feature_matrix_train_molecular.npy')
substring_features_train = np.load('feature_matrix_train_substring.npy')

# Concatenate the features
combined_features_train = np.concatenate((mol_features_train, substring_features_train), axis=1)

# Print the shape of the combined feature matrix
print("Combined features shape:", combined_features_train.shape)

# Save the combined feature matrix
np.save('feature_matrix_train.npy', combined_features_train)

import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Molecular representation
mol_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048)  # Modify parameters as needed
    mol_features_train.append(mol_features)

mol_features_train = np.array(mol_features_train)

# Print the shape of the molecular features
print("Molecular features shape:", mol_features_train.shape)

# Substring representation
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_train = np.array(substring_features_train)

# Print the shape of the substring features
print("Substring features shape:", substring_features_train.shape)

# Save the feature matrices
np.save('feature_matrix_train_molecular.npy', mol_features_train)
np.save('feature_matrix_train_substring.npy', substring_features_train)
import numpy as np

# Load the molecular and substring feature matrices
mol_features_train = np.load('feature_matrix_train_molecular.npy')
substring_features_train = np.load('feature_matrix_train_substring.npy')

# Concatenate the features
combined_features_train = np.concatenate((mol_features_train, substring_features_train), axis=1)

# Print the shape of the combined feature matrix
print("Combined features shape:", combined_features_train.shape)

# Save the combined feature matrix
np.save('feature_matrix_test.npy', combined_features_train)

"""5.4 GraphSAGE"""

!pip install networkx
!pip install igraph
!pip install spektral

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_train.npy', adjacency_matrix_train)

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_test.npy', adjacency_matrix_train)

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Reshape
from spektral.layers import GraphSageConv
from spektral.datasets import citation
from spektral.utils import normalized_adjacency
from scipy.sparse import coo_matrix
from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the adjacency matrix
adjacency_matrix_train = np.load('adjacency_matrix_train.npy')
adjacency_matrix_test = np.load('adjacency_matrix_test.npy')

# Convert adjacency matrix to SparseTensor
adjacency_matrix_train = coo_matrix(adjacency_matrix_train)
indices = np.column_stack((adjacency_matrix_train.row, adjacency_matrix_train.col))
adjacency_matrix_train = tf.sparse.SparseTensor(indices, adjacency_matrix_train.data, adjacency_matrix_train.shape)

adjacency_matrix_test = coo_matrix(adjacency_matrix_test)
indices = np.column_stack((adjacency_matrix_test.row, adjacency_matrix_test.col))
adjacency_matrix_test = tf.sparse.SparseTensor(indices, adjacency_matrix_test.data, adjacency_matrix_test.shape)

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features,)

# Define the GraphSAGE model architecture
x_in = tf.keras.Input(shape=input_shape)
a_in = tf.keras.Input(shape=(None,), sparse=True)

x = GraphSageConv(64, activation='relu')([x_in, a_in])
x = Dropout(0.5)(x)
x = GraphSageConv(64, activation='relu')([x, a_in])
x = Dropout(0.5)(x)
x = tf.keras.layers.Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(units=1, activation='sigmoid')(x)

# Create the model
model = Model(inputs=[x_in, a_in], outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x=[feature_matrix_train, adjacency_matrix_train], y=labels_train, batch_size=32, epochs=2000, validation_data=([feature_matrix_test, adjacency_matrix_test], labels_test))

# Evaluate the model
y_pred = model.predict([feature_matrix_test, adjacency_matrix_test])
y_pred_binary = (y_pred >= 0.5).astype(int)
print("Classification Report:")
print(classification_report(labels_test, y_pred_binary))

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(labels_test, y_pred)
aupr = np.trapz(precision, recall)
#print(f"AUPR: {aupr}")

# Calculate AUROC
auroc = roc_auc_score(labels_test, y_pred)
print(f"AUROC: {auroc}")

"""EARLY STOPPING GRAPHSAGE"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Reshape
from spektral.layers import GraphSageConv
from spektral.datasets import citation
from spektral.utils import normalized_adjacency
from scipy.sparse import coo_matrix
from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the adjacency matrix
adjacency_matrix_train = np.load('adjacency_matrix_train.npy')
adjacency_matrix_test = np.load('adjacency_matrix_test.npy')

# Convert adjacency matrix to SparseTensor
adjacency_matrix_train = coo_matrix(adjacency_matrix_train)
indices = np.column_stack((adjacency_matrix_train.row, adjacency_matrix_train.col))
adjacency_matrix_train = tf.sparse.SparseTensor(indices, adjacency_matrix_train.data, adjacency_matrix_train.shape)

adjacency_matrix_test = coo_matrix(adjacency_matrix_test)
indices = np.column_stack((adjacency_matrix_test.row, adjacency_matrix_test.col))
adjacency_matrix_test = tf.sparse.SparseTensor(indices, adjacency_matrix_test.data, adjacency_matrix_test.shape)

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features,)

# Define the GraphSAGE model architecture
x_in = tf.keras.Input(shape=input_shape)
a_in = tf.keras.Input(shape=(None,), sparse=True)

x = GraphSageConv(64, activation='relu')([x_in, a_in])
x = Dropout(0.5)(x)
x = GraphSageConv(64, activation='relu')([x, a_in])
x = Dropout(0.5)(x)
x = tf.keras.layers.Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(units=1, activation='sigmoid')(x)

# Create the model
model = Model(inputs=[x_in, a_in], outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(x=[feature_matrix_train, adjacency_matrix_train], y=labels_train, batch_size=32, epochs=2000,
          validation_data=([feature_matrix_test, adjacency_matrix_test], labels_test),
          callbacks=[early_stopping])

# Evaluate the model
y_pred = model.predict([feature_matrix_test, adjacency_matrix_test])
y_pred_binary = (y_pred >= 0.5).astype(int)
print("Classification Report:")
print(classification_report(labels_test, y_pred_binary))

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(labels_test, y_pred)
aupr = np.trapz(precision, recall)
#print(f"AUPR: {aupr}")

# Calculate AUROC
auroc = roc_auc_score(labels_test, y_pred)
print(f"AUROC: {auroc}")

"""GAT"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GAT model
class GAT(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1):
        super(GAT, self).__init__()
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, output_dim, heads=1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GAT model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2

# Create an instance of the GAT model
model = GAT(input_dim, hidden_dim, output_dim, num_heads)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GAT"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
from torch.nn import TransformerEncoder, TransformerEncoderLayer
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as edge_index tensors
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i] for i in range(num_nodes_train)], dtype=torch.long)

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i] for i in range(num_nodes_test)], dtype=torch.long)

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test)

# Define the GAT-Transformer hybrid model
class GATTransformer(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1, num_layers=1):
        super(GATTransformer, self).__init__()

        # GAT component
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1)

        # Transformer component
        self.embedding = nn.Linear(input_dim, hidden_dim)
        encoder_layers = TransformerEncoderLayer(hidden_dim, num_heads)
        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)

        # Output layer
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        # GAT component
        x_gat = self.embedding(x)
        x_gat = F.relu(self.conv1(x_gat, edge_index))
        x_gat = self.conv2(x_gat, edge_index)

        # Transformer component
        x_trans = x_gat.permute(1, 0, 2)  # Adjust dimensions for transformer input
        x_trans = self.transformer_encoder(x_trans)
        x_trans = x_trans.permute(1, 0, 2)  # Adjust dimensions back to original

        # Output layer
        x_out = self.fc(x_trans[:, -1, :])  # Take the last hidden state and pass it through a fully connected layer
        return x_out

# Define the dimensions of the GAT-Transformer model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2
num_layers = 2

# Create an instance of the GAT-Transformer model
model = GATTransformer(input_dim, hidden_dim, output_dim, num_heads, num_layers)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)

    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)

    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()

    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)

    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

"""GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Import the required modules for the Transformer
import torch.nn.Transformer as nnTransformer
import torch.nn.TransformerEncoder as nnTransformerEncoder
import torch.nn.TransformerEncoderLayer as nnTransformerEncoderLayer

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model with Transformers
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nnTransformerEncoderLayer(input_dim, hidden_dim),
            nn.ReLU(),
            nnTransformerEncoderLayer(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nnTransformerEncoderLayer(hidden_dim, hidden_dim),
            nn.ReLU(),
            nnTransformerEncoderLayer(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

"""GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv

# Load the feature matrix from the file
feature_matrix_test = np.load('feature_matrix_test.npy')

# Define the adjacency matrix (graph structure)
adjacency_matrix = np.eye(feature_matrix_test.shape[0])  # Assuming an identity adjacency matrix

# Convert the feature matrix and adjacency matrix to PyTorch tensors
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
adjacency_matrix = torch.from_numpy(adjacency_matrix).float()

# Create the edge_index tensor
num_nodes = feature_matrix_test.shape[0]
edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()

# Create a PyTorch Geometric data object
data = Data(x=feature_matrix_test, edge_index=edge_index, edge_attr=adjacency_matrix)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_test.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Perform a forward pass to obtain the node embeddings
node_embeddings = model(data.x, data.edge_index)

# Print the shape of the node embeddings
#print(f"Node embeddings shape: {node_embeddings.shape}")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Import the required modules for the Transformer
import torch.nn.Transformer as nnTransformer
import torch.nn.TransformerEncoder as nnTransformerEncoder
import torch.nn.TransformerEncoderLayer as nnTransformerEncoderLayer

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model with Transformers
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()

        self.transformer_encoder = nnTransformerEncoder(
            nnTransformerEncoderLayer(input_dim, hidden_dim),
            num_layers=2
        )
        self.linear = nn.Linear(input_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.transformer_encoder(x)
        x = F.relu(x)
        x = self.linear(x)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

"""6. THREE LEVEL GRANULARITY : FUSION FEATURES"""

import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Atomic representation
atomic_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    atomic_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)  # Modify parameters as needed
    atomic_features_train.append(atomic_features)

atomic_features_train = np.array(atomic_features_train)

# Substring representation (using n-grams)
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_train = np.array(substring_features_train)

# Molecular representation
mol_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048)  # Modify parameters as needed
    mol_features_train.append(mol_features)

mol_features_train = np.array(mol_features_train)

# Combine the feature matrices
feature_matrix_train = np.concatenate((atomic_features_train, substring_features_train, mol_features_train), axis=1)

# Print the shape of the feature matrix
print("Feature matrix shape:", feature_matrix_train.shape)

# Save the feature matrix to a .npy file
np.save('feature_matrix_train.npy', feature_matrix_train)

import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Atomic representation
atomic_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    atomic_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)  # Modify parameters as needed
    atomic_features_train.append(atomic_features)

atomic_features_train = np.array(atomic_features_train)

# Substring representation (using n-grams)
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_train = np.array(substring_features_train)

# Molecular representation
mol_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048)  # Modify parameters as needed
    mol_features_train.append(mol_features)

mol_features_train = np.array(mol_features_train)

# Combine the feature matrices
feature_matrix_train = np.concatenate((atomic_features_train, substring_features_train, mol_features_train), axis=1)

# Print the shape of the feature matrix
print("Feature matrix shape:", feature_matrix_train.shape)

# Save the feature matrix to a .npy file
np.save('feature_matrix_test.npy', feature_matrix_train)

"""6.2 Graph Methods : GraphSAGE"""

!pip install networkx
!pip install igraph
!pip install spektral

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_train.npy', adjacency_matrix_train)

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_test.npy', adjacency_matrix_train)

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Reshape
from spektral.layers import GraphSageConv
from spektral.datasets import citation
from spektral.utils import normalized_adjacency
from scipy.sparse import coo_matrix
from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the adjacency matrix
adjacency_matrix_train = np.load('adjacency_matrix_train.npy')
adjacency_matrix_test = np.load('adjacency_matrix_test.npy')

# Convert adjacency matrix to SparseTensor
adjacency_matrix_train = coo_matrix(adjacency_matrix_train)
indices = np.column_stack((adjacency_matrix_train.row, adjacency_matrix_train.col))
adjacency_matrix_train = tf.sparse.SparseTensor(indices, adjacency_matrix_train.data, adjacency_matrix_train.shape)

adjacency_matrix_test = coo_matrix(adjacency_matrix_test)
indices = np.column_stack((adjacency_matrix_test.row, adjacency_matrix_test.col))
adjacency_matrix_test = tf.sparse.SparseTensor(indices, adjacency_matrix_test.data, adjacency_matrix_test.shape)

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features,)

# Define the GraphSAGE model architecture
x_in = tf.keras.Input(shape=input_shape)
a_in = tf.keras.Input(shape=(None,), sparse=True)

x = GraphSageConv(64, activation='relu')([x_in, a_in])
x = Dropout(0.5)(x)
x = GraphSageConv(64, activation='relu')([x, a_in])
x = Dropout(0.5)(x)
x = tf.keras.layers.Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(units=1, activation='sigmoid')(x)

# Create the model
model = Model(inputs=[x_in, a_in], outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x=[feature_matrix_train, adjacency_matrix_train], y=labels_train, batch_size=32, epochs=2000, validation_data=([feature_matrix_test, adjacency_matrix_test], labels_test))

# Evaluate the model
y_pred = model.predict([feature_matrix_test, adjacency_matrix_test])
y_pred_binary = (y_pred >= 0.5).astype(int)
print("Classification Report:")
print(classification_report(labels_test, y_pred_binary))

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(labels_test, y_pred)
aupr = np.trapz(precision, recall)
#print(f"AUPR: {aupr}")

# Calculate AUROC
auroc = roc_auc_score(labels_test, y_pred)
print(f"AUROC: {auroc}")

# Calculate specificity

"""EARLY STOPPING GRAPHSAGE"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Reshape
from spektral.layers import GraphSageConv
from spektral.datasets import citation
from spektral.utils import normalized_adjacency
from scipy.sparse import coo_matrix
from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the adjacency matrix
adjacency_matrix_train = np.load('adjacency_matrix_train.npy')
adjacency_matrix_test = np.load('adjacency_matrix_test.npy')

# Convert adjacency matrix to SparseTensor
adjacency_matrix_train = coo_matrix(adjacency_matrix_train)
indices = np.column_stack((adjacency_matrix_train.row, adjacency_matrix_train.col))
adjacency_matrix_train = tf.sparse.SparseTensor(indices, adjacency_matrix_train.data, adjacency_matrix_train.shape)

adjacency_matrix_test = coo_matrix(adjacency_matrix_test)
indices = np.column_stack((adjacency_matrix_test.row, adjacency_matrix_test.col))
adjacency_matrix_test = tf.sparse.SparseTensor(indices, adjacency_matrix_test.data, adjacency_matrix_test.shape)

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features,)

# Define the GraphSAGE model architecture
x_in = tf.keras.Input(shape=input_shape)
a_in = tf.keras.Input(shape=(None,), sparse=True)

x = GraphSageConv(64, activation='relu')([x_in, a_in])
x = Dropout(0.5)(x)
x = GraphSageConv(64, activation='relu')([x, a_in])
x = Dropout(0.5)(x)
x = tf.keras.layers.Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(units=1, activation='sigmoid')(x)

# Create the model
model = Model(inputs=[x_in, a_in], outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model with early stopping
model.fit(x=[feature_matrix_train, adjacency_matrix_train], y=labels_train, batch_size=32, epochs=2000,
          validation_data=([feature_matrix_test, adjacency_matrix_test], labels_test),
          callbacks=[early_stopping])

# Evaluate the model
y_pred = model.predict([feature_matrix_test, adjacency_matrix_test])
y_pred_binary = (y_pred >= 0.5).astype(int)
print("Classification Report:")
print(classification_report(labels_test, y_pred_binary))

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(labels_test, y_pred)
aupr = np.trapz(precision, recall)
#print(f"AUPR: {aupr}")

# Calculate AUROC
auroc = roc_auc_score(labels_test, y_pred)
print(f"AUROC: {auroc}")

"""GAT"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GAT model
class GAT(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1):
        super(GAT, self).__init__()
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, output_dim, heads=1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GAT model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2

# Create an instance of the GAT model
model = GAT(input_dim, hidden_dim, output_dim, num_heads)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GAT"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
from torch.nn import TransformerEncoder, TransformerEncoderLayer
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as edge_index tensors
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i] for i in range(num_nodes_train)], dtype=torch.long)

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i] for i in range(num_nodes_test)], dtype=torch.long)

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test)

# Define the GAT-Transformer hybrid model
class GATTransformer(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1, num_layers=1):
        super(GATTransformer, self).__init__()

        # GAT component
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1)

        # Transformer component
        self.embedding = nn.Linear(input_dim, hidden_dim)
        encoder_layers = TransformerEncoderLayer(hidden_dim, num_heads)
        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)

        # Output layer
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        # GAT component
        x_gat = self.embedding(x)
        x_gat = F.relu(self.conv1(x_gat, edge_index))
        x_gat = self.conv2(x_gat, edge_index)

        # Transformer component
        x_trans = x_gat.permute(1, 0, 2)  # Adjust dimensions for transformer input
        x_trans = self.transformer_encoder(x_trans)
        x_trans = x_trans.permute(1, 0, 2)  # Adjust dimensions back to original

        # Output layer
        x_out = self.fc(x_trans[:, -1, :])  # Take the last hidden state and pass it through a fully connected layer
        return x_out

# Define the dimensions of the GAT-Transformer model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2
num_layers = 2

# Create an instance of the GAT-Transformer model
model = GATTransformer(input_dim, hidden_dim, output_dim, num_heads, num_layers)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)

    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)

    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()

    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)

    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

"""GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Import the required modules for the Transformer
import torch.nn.Transformer as nnTransformer
import torch.nn.TransformerEncoder as nnTransformerEncoder
import torch.nn.TransformerEncoderLayer as nnTransformerEncoderLayer

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model with Transformers
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nnTransformerEncoderLayer(input_dim, hidden_dim),
            nn.ReLU(),
            nnTransformerEncoderLayer(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nnTransformerEncoderLayer(hidden_dim, hidden_dim),
            nn.ReLU(),
            nnTransformerEncoderLayer(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

"""GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv

# Load the feature matrix from the file
feature_matrix_test = np.load('feature_matrix_test.npy')

# Define the adjacency matrix (graph structure)
adjacency_matrix = np.eye(feature_matrix_test.shape[0])  # Assuming an identity adjacency matrix

# Convert the feature matrix and adjacency matrix to PyTorch tensors
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
adjacency_matrix = torch.from_numpy(adjacency_matrix).float()

# Create the edge_index tensor
num_nodes = feature_matrix_test.shape[0]
edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()

# Create a PyTorch Geometric data object
data = Data(x=feature_matrix_test, edge_index=edge_index, edge_attr=adjacency_matrix)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_test.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Perform a forward pass to obtain the node embeddings
node_embeddings = model(data.x, data.edge_index)

# Print the shape of the node embeddings
#print(f"Node embeddings shape: {node_embeddings.shape}")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GRAPHORMER GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Import the required modules for the Transformer
import torch.nn.Transformer as nnTransformer
import torch.nn.TransformerEncoder as nnTransformerEncoder
import torch.nn.TransformerEncoderLayer as nnTransformerEncoderLayer

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model with Transformers
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()

        self.transformer_encoder = nnTransformerEncoder(
            nnTransformerEncoderLayer(input_dim, hidden_dim),
            num_layers=2
        )
        self.linear = nn.Linear(input_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.transformer_encoder(x)
        x = F.relu(x)
        x = self.linear(x)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

from sklearn.metrics import classification_report

# Convert the predicted labels and target labels to numpy arrays
predicted_labels_test = predicted_labels_test.numpy()
target_labels_test = target_labels_test.numpy()

# Generate the classification report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

