# -*- coding: utf-8 -*-
"""Codscomad.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LeuT9Td6IPt2BWsktseSjhaG5SBXEgWF
"""

!pip install --pre deepchem
!pip install rdkit-pypi
from rdkit import rdBase
from rdkit import RDConfig
!pip install --pre deepchem
!pip install rdkit-pypi
!pip install rdkit-pypi
!pip install mordred
!pip install --pre deepchem
!pip install rdkit-pypi
import deepchem as dc
dc.__version__

!pip install torch_geometric

!pip install torch

import pandas as pd
import numpy as np
from google.colab import drive
drive.mount('/content/drive')
dataset = pd.read_csv('/content/drive/MyDrive/Resources for AI-23/sr/smell/training_data.csv')
dataset1 = pd.read_csv('/content/drive/MyDrive/Resources for AI-23/sr/smell/testing_data.csv')

print(dataset.shape)

print(dataset1.shape)

dataset.head()

"""Training data preprocessing"""

import pandas as pd

# Read the CSV file
df = pd.read_csv('/content/drive/MyDrive/Resources for AI-23/sr/smell/training_data.csv')

# Remove rows with NaN values
df_cleaned_train = df.dropna()

# Print the cleaned DataFrame
print(df_cleaned_train)

import pandas as pd
# Convert the DataFrame to CSV format
df_cleaned_train.to_csv('cleaned_data_train.csv', index=False)

import pandas as pd
from rdkit import Chem

# Read the CSV file containing the data
df = pd.read_csv('cleaned_data_train.csv')

# Remove invalid SMILES
valid_indices = []
for i, smiles in enumerate(df['SMILES']):
    mol = Chem.MolFromSmiles(smiles)
    if mol is not None:
        valid_indices.append(i)

# Filter the DataFrame using the valid indices
df_cleaned = df.iloc[valid_indices]

# Save the cleaned DataFrame to a new CSV file
df_cleaned.to_csv('cleaned_train.csv', index=False)

print(df_cleaned.shape)

"""Test data preprocessing"""

import pandas as pd

# Read the CSV file
df = pd.read_csv('/content/drive/MyDrive/Resources for AI-23/sr/smell/testing_data.csv')

# Remove rows with NaN values
df_cleaned_train = df.dropna()

# Print the cleaned DataFrame
print(df_cleaned_train)

import pandas as pd


df_cleaned_train.to_csv('cleaned_data_test.csv', index=False)

import pandas as pd
from rdkit import Chem

# Read the CSV file containing the data
df = pd.read_csv('cleaned_data_test.csv')

# Remove invalid SMILES
valid_indices = []
for i, smiles in enumerate(df['SMILES']):
    mol = Chem.MolFromSmiles(smiles)
    if mol is not None:
        valid_indices.append(i)

# Filter the DataFrame using the valid indices
df_cleaned = df.iloc[valid_indices]

# Save the cleaned DataFrame to a new CSV file
df_cleaned.to_csv('cleaned_test.csv', index=False)

print(df_cleaned.shape)

"""a.) ATOMIC REPRESENTATIONS"""

import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem

# Read the DataFrame containing the  dataset
df_cleaned_train = pd.read_csv('cleaned_train.csv')

# Create a new column to store the atomic representations
df_cleaned_train['Atomic_Representations'] = None

# Iterate over the SMILES strings and generate atomic representations
for i, smiles in enumerate(df_cleaned_train['SMILES']):
    mol = Chem.MolFromSmiles(smiles)  # Convert SMILES to RDKit molecule object
    if mol is not None:
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2)  # Generate Morgan fingerprint
        df_cleaned_train.at[i, 'Atomic_Representations'] = fp

# Save the updated DataFrame with atomic representations
df_cleaned_train.to_csv('stat_dataset_train_with_atomic.csv', index=False)

import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem import AllChem

# Read the DataFrame containing the  dataset
df_cleaned_train = pd.read_csv('cleaned_train.csv')

# Create an empty list to store the feature vectors
feature_vectors = []

# Iterate over the SMILES strings and generate atomic representations
for smiles in df_cleaned_train['SMILES']:
    mol = Chem.MolFromSmiles(smiles)  # Convert SMILES to RDKit molecule object
    if mol is not None:
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2)  # Generate Morgan fingerprint
        feature_vector = np.array(list(fp), dtype=int)  # Convert fingerprint to numpy array
        feature_vectors.append(feature_vector)

# Convert the list of feature vectors to a numpy array
feature_matrix = np.array(feature_vectors)

# Save the feature matrix to a file
np.save('feature_matrix_train.npy', feature_matrix)

# Load the feature matrix from the file
feature_matrix_train = np.load('feature_matrix_train.npy')

# Get the shape of the feature matrix
num_samples, num_features = feature_matrix_train.shape
print(f"Number of samples: {num_samples}")
print(f"Number of features: {num_features}")

# Access a specific feature vector
sample_index = 0
feature_vector = feature_matrix_train[sample_index]
print(f"Feature vector for sample {sample_index}: {feature_vector}")

# Load the feature matrix from the file
feature_matrix_train = np.load('feature_matrix_train.npy')

# Iterate over the feature matrix and print each feature vector
for i, feature_vector in enumerate(feature_matrix_train):
    print(f"Feature vector for sample {i}: {feature_vector}")

print(feature_matrix_train.shape)

import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem


df_cleaned_train = pd.read_csv('cleaned_test.csv')

# Create a new column to store the atomic representations
df_cleaned_train['Atomic_Representations'] = None

# Iterate over the SMILES strings and generate atomic representations
for i, smiles in enumerate(df_cleaned_train['SMILES']):
    mol = Chem.MolFromSmiles(smiles)  # Convert SMILES to RDKit molecule object
    if mol is not None:
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2)  # Generate Morgan fingerprint
        df_cleaned_train.at[i, 'Atomic_Representations'] = fp

# Save the updated DataFrame with atomic representations
df_cleaned_train.to_csv('stat_dataset_test_with_atomic.csv', index=False)

import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem import AllChem

# Read the DataFrame containing the dataset
df_cleaned_train = pd.read_csv('cleaned_test.csv')

# Create an empty list to store the feature vectors
feature_vectors = []

# Iterate over the SMILES strings and generate atomic representations
for smiles in df_cleaned_train['SMILES']:
    mol = Chem.MolFromSmiles(smiles)  # Convert SMILES to RDKit molecule object
    if mol is not None:
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2)  # Generate Morgan fingerprint
        feature_vector = np.array(list(fp), dtype=int)  # Convert fingerprint to numpy array
        feature_vectors.append(feature_vector)

# Convert the list of feature vectors to a numpy array
feature_matrix = np.array(feature_vectors)

# Save the feature matrix to a file
np.save('feature_matrix_test.npy', feature_matrix)

# Load the feature matrix from the file (optional if you have not restarted the Python session)
feature_matrix_test = np.load('feature_matrix_test.npy')

# Get the shape of the feature matrix
num_samples, num_features = feature_matrix.shape
print(f"Number of samples: {num_samples}")
print(f"Number of features: {num_features}")

# Access a specific feature vector
sample_index = 0
feature_vector = feature_matrix[sample_index]
print(f"Feature vector for sample {sample_index}: {feature_vector}")

# Get the shape of the feature matrix
num_samples, num_features = feature_matrix_test.shape
print(f"Number of samples: {num_samples}")
print(f"Number of features: {num_features}")
import numpy as np

# Load the feature matrix from the file
feature_matrix_test = np.load('feature_matrix_test.npy')

# Get the shape of the feature matrix
num_samples, num_features = feature_matrix_test.shape
print(f"Number of samples: {num_samples}")
print(f"Number of features: {num_features}")

# Access a specific feature vector
sample_index = 0
feature_vector = feature_matrix_test[sample_index]
print(f"Feature vector for sample {sample_index}: {feature_vector}")

feature_matrix_test = np.load('feature_matrix_test.npy')

# Iterate over the feature matrix and print each feature vector
for i, feature_vector in enumerate(feature_matrix):
    print(f"Feature vector for sample {i}: {feature_vector}")

print(feature_matrix_test.shape)

"""DL METHODS ON ATOMIC COORDINATES

1.1 Model #DNN
"""

feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the training labels from the input CSV file
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values  # Assuming the label column is named 'Label'

feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the test labels from the input CSV file
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values  # Assuming the label column is named 'Label'

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Define the model architecture
model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(num_features,)))
model.add(Dropout(0.5))  # Adding dropout regularization
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))  # Adding dropout regularization
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model with modified parameters
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=50, validation_data=(feature_matrix_test, labels_test))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert the predicted probabilities to binary predictions
binary_predictions = (predictions > 0.5).astype(int)

# Compute accuracy
accuracy = accuracy_score(labels_test, binary_predictions)
print(f"Accuracy: {accuracy}")

# Compute precision
precision = precision_score(labels_test, binary_predictions)
print(f"Precision: {precision}")

# Compute recall
recall = recall_score(labels_test, binary_predictions)
print(f"Recall: {recall}")

# Compute F1 score
f1 = f1_score(labels_test, binary_predictions)
print(f"F1 Score: {f1}")

from sklearn.metrics import average_precision_score, confusion_matrix

aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

import numpy as np
import pandas as pd
from sklearn.metrics import roc_auc_score, confusion_matrix

# Load the training labels from the input CSV file
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test labels from the input CSV file
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# ...

# Train the model with modified parameters
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=50, validation_data=(feature_matrix_test, labels_test))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Compute confusion matrix
binary_predictions = (predictions > 0.5).astype(int)
cm = confusion_matrix(labels_test, binary_predictions)
tn, fp, fn, tp = cm.ravel()

# Compute specificity
specificity = tn / (tn + fp)
print(f"Specificity: {specificity}")

# Compute sensitivity (recall)
sensitivity = tp / (tp + fn)
print(f"Sensitivity/Recall: {sensitivity}")

# Compute AUROC
auroc = roc_auc_score(labels_test, predictions)
print(f"AUROC: {auroc}")

# Calculate AUPR
from sklearn.metrics import average_precision_score, confusion_matrix
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Early Stopping DNN"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Define the model architecture
model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(num_features,)))
model.add(Dropout(0.5))  # Adding dropout regularization
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))  # Adding dropout regularization
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=2000,
          validation_data=(feature_matrix_test, labels_test),
          callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Save the model weights
model.save_weights('model_weights.h5')

"""1.2 Model -LSTM"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Dropout, Reshape

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features,)

# Reshape the feature matrix to have a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, num_features))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, num_features))

# Define the Bi-LSTM model architecture
model = Sequential()
model.add(Bidirectional(LSTM(64, activation='relu'), input_shape=(1, num_features)))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=100, validation_data=(feature_matrix_test, labels_test))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

from sklearn.metrics import classification_report

# Threshold predictions to binary values
binary_predictions = (predictions > 0.5).astype(int)

# Generate classification report
report = classification_report(labels_test, binary_predictions)
print(report)
# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Early stopping LSTM"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Dropout, Reshape
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features,)

# Reshape the feature matrix to have a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, num_features))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, num_features))

# Define the Bi-LSTM model architecture
model = Sequential()
model.add(Bidirectional(LSTM(64, activation='relu'), input_shape=(1, num_features)))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=2000,
          validation_data=(feature_matrix_test, labels_test),
          callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Save the model weights
model.save_weights('model_weights.h5')

"""1.3 RNN"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, SimpleRNN, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Normalize the feature matrices
scaler = StandardScaler()
feature_matrix_train = scaler.fit_transform(feature_matrix_train)
feature_matrix_test = scaler.transform(feature_matrix_test)

# Reshape the feature matrices to include the sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Define the RNN model architecture (SimpleRNN)
model = Sequential()
model.add(SimpleRNN(128, activation='relu', input_shape=(1, feature_matrix_train.shape[2])))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(feature_matrix_train, labels_train, test_size=0.2, random_state=42)

# Train the model
model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_val, y_val))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

from sklearn.metrics import classification_report

# Threshold predictions to binary values
binary_predictions = (predictions > 0.5).astype(int)

# Generate classification report
report = classification_report(labels_test, binary_predictions)
print(report)
# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Early Stopping RNN"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, SimpleRNN, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Normalize the feature matrices
scaler = StandardScaler()
feature_matrix_train = scaler.fit_transform(feature_matrix_train)
feature_matrix_test = scaler.transform(feature_matrix_test)

# Reshape the feature matrices to include the sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Define the RNN model architecture (SimpleRNN)
model = Sequential()
model.add(SimpleRNN(128, activation='relu', input_shape=(1, feature_matrix_train.shape[2])))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(feature_matrix_train, labels_train, test_size=0.2, random_state=42)

# Define early stopping callback
early_stopping = EarlyStopping(patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(X_train, y_train, batch_size=32, epochs=2000, validation_data=(X_val, y_val), callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Save the model weights
model.save_weights('model_weights.h5')

"""1.4 Bi-RNN"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, SimpleRNN, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Normalize the feature matrices
scaler = StandardScaler()
feature_matrix_train = scaler.fit_transform(feature_matrix_train)
feature_matrix_test = scaler.transform(feature_matrix_test)

# Reshape the feature matrices to include the sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Define the bi-RNN model architecture
model = Sequential()
model.add(Bidirectional(SimpleRNN(128, activation='relu'), input_shape=(1, feature_matrix_train.shape[2])))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(feature_matrix_train, labels_train, test_size=0.2, random_state=42)

# Train the model
model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_val, y_val))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

from sklearn.metrics import classification_report

# Threshold predictions to binary values
binary_predictions = (predictions > 0.5).astype(int)

# Generate classification report
report = classification_report(labels_test, binary_predictions)
print(report)
# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Early Stopping Bi-RNN"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, SimpleRNN, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Normalize the feature matrices
scaler = StandardScaler()
feature_matrix_train = scaler.fit_transform(feature_matrix_train)
feature_matrix_test = scaler.transform(feature_matrix_test)

# Reshape the feature matrices to include the sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Define the bi-RNN model architecture
model = Sequential()
model.add(Bidirectional(SimpleRNN(128, activation='relu'), input_shape=(1, feature_matrix_train.shape[2])))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(feature_matrix_train, labels_train, test_size=0.2, random_state=42)

# Define early stopping callback
early_stopping = EarlyStopping(patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(X_train, y_train, batch_size=32, epochs=2000, validation_data=(X_val, y_val), callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Save the model weights
model.save_weights('model_weights.h5')

"""1.5 Bi-GRU"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, GRU, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Normalize the feature matrices
scaler = StandardScaler()
feature_matrix_train = scaler.fit_transform(feature_matrix_train)
feature_matrix_test = scaler.transform(feature_matrix_test)

# Reshape the feature matrices to include the sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
sequence_length_test = feature_matrix_test.shape[0]
num_features = feature_matrix_train.shape[1]

feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, num_features))
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, num_features))

# Define the bi-GRU model architecture
model = Sequential()
model.add(Bidirectional(GRU(128, activation='relu'), input_shape=(1, num_features)))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(feature_matrix_train, labels_train, test_size=0.2, random_state=42)

# Train the model
model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_val, y_val))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

from sklearn.metrics import classification_report

# Threshold predictions to binary values
binary_predictions = (predictions > 0.5).astype(int)

# Generate classification report
report = classification_report(labels_test, binary_predictions)
print(report)
from sklearn.metrics import average_precision_score

# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Early Stopping Bi-GRU"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, GRU, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Normalize the feature matrices
scaler = StandardScaler()
feature_matrix_train = scaler.fit_transform(feature_matrix_train)
feature_matrix_test = scaler.transform(feature_matrix_test)

# Reshape the feature matrices to include the sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
sequence_length_test = feature_matrix_test.shape[0]
num_features = feature_matrix_train.shape[1]

feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, num_features))
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, num_features))

# Define the bi-GRU model architecture
model = Sequential()
model.add(Bidirectional(GRU(128, activation='relu'), input_shape=(1, num_features)))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(feature_matrix_train, labels_train, test_size=0.2, random_state=42)

# Define early stopping callback
early_stopping = EarlyStopping(patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(X_train, y_train, batch_size=32, epochs=2000, validation_data=(X_val, y_val), callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Save the model weights
model.save_weights('model_weights.h5')

"""2. ML METHODS USING ATOMIC COORDINATES"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Initialize the Random Forest classifier
random_forest = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier
random_forest.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = random_forest.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

"""1. RF"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, average_precision_score

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Initialize the Random Forest classifier
random_forest = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier
random_forest.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = random_forest.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

# Generate classification report
report = classification_report(labels_test, predictions)
print("Classification Report:\n", report)
# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, precision_score, recall_score

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Initialize the Random Forest classifier
random_forest = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier
random_forest.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = random_forest.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

# Calculate precision, recall, and F1 score
precision = precision_score(labels_test, predictions)
recall = recall_score(labels_test, predictions)
f1 = 2 * (precision * recall) / (precision + recall)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

# Calculate AUROC
proba = random_forest.predict_proba(feature_matrix_test)[:, 1]
auroc = roc_auc_score(labels_test, proba)
print("AUROC:", auroc)

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Calculate sensitivity
sensitivity = recall
print("Sensitivity:", sensitivity)
# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Ensemble methods - Bagging, Boosting"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier
from sklearn.metrics import accuracy_score

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Initialize the base Random Forest classifier
base_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Bagging (Bootstrap Aggregating)
bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)
bagging_classifier.fit(feature_matrix_train, labels_train)
bagging_predictions = bagging_classifier.predict(feature_matrix_test)
bagging_accuracy = accuracy_score(labels_test, bagging_predictions)
print("Bagging Accuracy:", bagging_accuracy)

# Boosting (AdaBoost)
boosting_classifier = AdaBoostClassifier(base_classifier, n_estimators=10, random_state=42)
boosting_classifier.fit(feature_matrix_train, labels_train)
boosting_predictions = boosting_classifier.predict(feature_matrix_test)
boosting_accuracy = accuracy_score(labels_test, boosting_predictions)
print("Boosting Accuracy:", boosting_accuracy)

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier
from sklearn.metrics import accuracy_score, classification_report, average_precision_score

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Initialize the base Random Forest classifier
base_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Bagging (Bootstrap Aggregating)
bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)
bagging_classifier.fit(feature_matrix_train, labels_train)
bagging_predictions = bagging_classifier.predict(feature_matrix_test)
bagging_accuracy = accuracy_score(labels_test, bagging_predictions)
print("Bagging Accuracy:", bagging_accuracy)

# Generate classification report for Bagging
bagging_report = classification_report(labels_test, bagging_predictions)
print("Bagging Classification Report:\n", bagging_report)

# Boosting (AdaBoost)
boosting_classifier = AdaBoostClassifier(base_classifier, n_estimators=10, random_state=42)
boosting_classifier.fit(feature_matrix_train, labels_train)
boosting_predictions = boosting_classifier.predict(feature_matrix_test)
boosting_accuracy = accuracy_score(labels_test, boosting_predictions)
print("Boosting Accuracy:", boosting_accuracy)

# Generate classification report for Boosting
boosting_report = classification_report(labels_test, boosting_predictions)
print("Boosting Classification Report:\n", boosting_report)
# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, boosting_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""2. XGBoost"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import accuracy_score

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Initialize the XGBoost classifier
xgb_classifier = xgb.XGBClassifier()

# Train the classifier
xgb_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = xgb_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

# Calculate AUROC
probabilities = xgb_classifier.predict_proba(feature_matrix_test)[:, 1]
auroc = roc_auc_score(labels_test, probabilities)
print("AUROC:", auroc)

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)
# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, boosting_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""3. SVC"""

import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Initialize the SVC classifier
svc_classifier = SVC()

# Train the classifier
svc_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = svc_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Initialize the SVC classifier
svc_classifier = SVC()

# Train the classifier
svc_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = svc_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

# Calculate precision
precision = precision_score(labels_test, predictions)
print("Precision:", precision)

# Calculate recall
recall = recall_score(labels_test, predictions)
print("Recall:", recall)

# Calculate F1-score
f1 = f1_score(labels_test, predictions)
print("F1-score:", f1)

# Calculate ROC AUC
roc_auc = roc_auc_score(labels_test, predictions)
print("ROC AUC:", roc_auc)

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Initialize the XGBoost classifier
xgb_classifier = xgb.XGBClassifier()

# Train the classifier
xgb_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = xgb_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

# Calculate precision
precision = precision_score(labels_test, predictions)
print("Precision:", precision)

# Calculate recall
recall = recall_score(labels_test, predictions)
print("Recall:", recall)

# Calculate F1-score
f1 = f1_score(labels_test, predictions)
print("F1-score:", f1)

# Calculate confusion matrix
confusion_mat = confusion_matrix(labels_test, predictions)
print("Confusion Matrix:")
print(confusion_mat)
# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, boosting_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""GRAPH METHODS

1. GAT
"""

!pip install torch_geometric
!pip install torch

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GAT model
class GAT(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1):
        super(GAT, self).__init__()
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, output_dim, heads=1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GAT model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2

# Create an instance of the GAT model
model = GAT(input_dim, hidden_dim, output_dim, num_heads)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""2. GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv

# Load the feature matrix from the file
feature_matrix_test = np.load('feature_matrix_test.npy')

# Define the adjacency matrix (graph structure)
adjacency_matrix = np.eye(feature_matrix_test.shape[0])  # Assuming an identity adjacency matrix

# Convert the feature matrix and adjacency matrix to PyTorch tensors
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
adjacency_matrix = torch.from_numpy(adjacency_matrix).float()

# Create the edge_index tensor
num_nodes = feature_matrix_test.shape[0]
edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()

# Create a PyTorch Geometric data object
data = Data(x=feature_matrix_test, edge_index=edge_index, edge_attr=adjacency_matrix)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_test.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Perform a forward pass to obtain the node embeddings
node_embeddings = model(data.x, data.edge_index)

# Print the shape of the node embeddings
#print(f"Node embeddings shape: {node_embeddings.shape}")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""b.) SUBSTRING REPRESENATATIONS"""

import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Substring representation
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_train = np.array(substring_features_train)

# Print the shape of the substring features
print("Substring features shape:", substring_features_train.shape)
np.save('feature_matrix_train.npy', substring_features_train)

import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Substring representation
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_test = np.array(substring_features_train)

# Print the shape of the substring features
print("Substring features shape:", substring_features_test.shape)
# Save the substring features to a .npy file
np.save('feature_matrix_test.npy', substring_features_test)

# Perform further preprocessing or use the substring features for model training

"""2.1 Model  Bi-RNN"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, SimpleRNN, Dropout, Reshape

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the Bi-RNN model architecture
model = Sequential()
model.add(Bidirectional(SimpleRNN(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=100, validation_data=(feature_matrix_test, labels_test))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert the predicted probabilities to binary predictions
binary_predictions = (predictions > 0.5).astype(int)

# Compute accuracy
accuracy = accuracy_score(labels_test, binary_predictions)
print(f"Accuracy: {accuracy}")

# Compute precision
precision = precision_score(labels_test, binary_predictions)
print(f"Precision: {precision}")

# Compute recall
recall = recall_score(labels_test, binary_predictions)
print(f"Recall: {recall}")

# Compute F1 score
f1 = f1_score(labels_test, binary_predictions)
print(f"F1 Score: {f1}")

# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Early Stopping Bi-RNN"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, SimpleRNN, Dropout, Reshape
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the Bi-RNN model architecture
model = Sequential()
model.add(Bidirectional(SimpleRNN(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=2000, validation_data=(feature_matrix_test, labels_test), callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Save the model weights
model.save_weights('model_weights.h5')

"""2. Bi-LSTM"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Dropout, Reshape

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the Bi-LSTM model architecture
model = Sequential()
model.add(Bidirectional(LSTM(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=2000, validation_data=(feature_matrix_test, labels_test))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert the predicted probabilities to binary predictions
binary_predictions = (predictions > 0.5).astype(int)

# Compute accuracy
accuracy = accuracy_score(labels_test, binary_predictions)
print(f"Accuracy: {accuracy}")

# Compute precision
precision = precision_score(labels_test, binary_predictions)
print(f"Precision: {precision}")

# Compute recall
recall = recall_score(labels_test, binary_predictions)
print(f"Recall: {recall}")

# Compute F1 score
f1 = f1_score(labels_test, binary_predictions)
print(f"F1 Score: {f1}")

# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Early Stopping Bi-LSTM"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Dropout, Reshape
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the Bi-LSTM model architecture
model = Sequential()
model.add(Bidirectional(LSTM(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=2000, validation_data=(feature_matrix_test, labels_test), callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Save the model weights
model.save_weights('model_weights.h5')

"""2.3. RNN"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, SimpleRNN, Dropout, Reshape

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the bi-RNN model architecture
model = Sequential()
model.add(Bidirectional(SimpleRNN(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=50, validation_data=(feature_matrix_test, labels_test))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert the predicted probabilities to binary predictions
binary_predictions = (predictions > 0.5).astype(int)

# Compute accuracy
accuracy = accuracy_score(labels_test, binary_predictions)
print(f"Accuracy: {accuracy}")

# Compute precision
precision = precision_score(labels_test, binary_predictions)
print(f"Precision: {precision}")

# Compute recall
recall = recall_score(labels_test, binary_predictions)
print(f"Recall: {recall}")

# Compute F1 score
f1 = f1_score(labels_test, binary_predictions)
print(f"F1 Score: {f1}")

# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Early Stopping RNN"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, SimpleRNN, Dropout, Reshape
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the bi-RNN model architecture
model = Sequential()
model.add(Bidirectional(SimpleRNN(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=2000, validation_data=(feature_matrix_test, labels_test), callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Save the model weights
model.save_weights('model_weights.h5')

"""2.4. Bi-GRU"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, GRU, Dropout, Reshape

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the bi-GRU model architecture
model = Sequential()
model.add(Bidirectional(GRU(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=50, validation_data=(feature_matrix_test, labels_test))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert the predicted probabilities to binary predictions
binary_predictions = (predictions > 0.5).astype(int)

# Compute accuracy
accuracy = accuracy_score(labels_test, binary_predictions)
print(f"Accuracy: {accuracy}")

# Compute precision
precision = precision_score(labels_test, binary_predictions)
print(f"Precision: {precision}")

# Compute recall
recall = recall_score(labels_test, binary_predictions)
print(f"Recall: {recall}")

# Compute F1 score
f1 = f1_score(labels_test, binary_predictions)
print(f"F1 Score: {f1}")

# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Early Stopping Bi-GRU"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, GRU, Dropout, Reshape
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the bi-GRU model architecture
model = Sequential()
model.add(Bidirectional(GRU(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=2000, validation_data=(feature_matrix_test, labels_test), callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Save the model weights
model.save_weights('model_weights.h5')

"""GRAPH METHODS

1. GAT
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GAT model
class GAT(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1):
        super(GAT, self).__init__()
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, output_dim, heads=1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GAT model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2

# Create an instance of the GAT model
model = GAT(input_dim, hidden_dim, output_dim, num_heads)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""2. GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""3. GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv

# Load the feature matrix from the file
feature_matrix_test = np.load('feature_matrix_test.npy')

# Define the adjacency matrix (graph structure)
adjacency_matrix = np.eye(feature_matrix_test.shape[0])  # Assuming an identity adjacency matrix

# Convert the feature matrix and adjacency matrix to PyTorch tensors
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
adjacency_matrix = torch.from_numpy(adjacency_matrix).float()

# Create the edge_index tensor
num_nodes = feature_matrix_test.shape[0]
edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()

# Create a PyTorch Geometric data object
data = Data(x=feature_matrix_test, edge_index=edge_index, edge_attr=adjacency_matrix)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_test.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Perform a forward pass to obtain the node embeddings
node_embeddings = model(data.x, data.edge_index)

# Print the shape of the node embeddings
#print(f"Node embeddings shape: {node_embeddings.shape}")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""## c.) MOLECULAR GRAPHS"""

from rdkit import Chem
from rdkit.Chem import Draw
import pandas as pd

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
smiles_train = df_train['SMILES'].values

# Generate RDKit molecules from SMILES strings
mols_train = [Chem.MolFromSmiles(smiles) for smiles in smiles_train]

# Generate molecular images
images = [Draw.MolToImage(mol) for mol in mols_train]

# Display the molecular images
for image in images:
    image.show()

from rdkit import Chem
from rdkit.Chem import Draw
import pandas as pd

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')
smiles_train = df_train['SMILES'].values

# Generate RDKit molecules from SMILES strings
mols_train = [Chem.MolFromSmiles(smiles) for smiles in smiles_train]

# Generate molecular images
images = [Draw.MolToImage(mol) for mol in mols_train]

# Display the molecular images
for image in images:
    image.show()

"""Molecular Represenation where we use RDKit to convert the SMILES to molecular objects and generate molecular images, which are saved as separate image files.

Next, we calculate several molecular descriptors for each molecule and generates a molecular fingerprint using the Morgan fingerprint algorithm. The descriptors and fingerprint are combined into a feature vector, and the feature vectors are collected to form a feature matrix.
"""

import pandas as pd
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem

# Read the SMILES from the cleaned_train.csv file
data = pd.read_csv('cleaned_train.csv')
smiles = data['SMILES']

# Generate molecular image representation
mols = [Chem.MolFromSmiles(smile) for smile in smiles]
images = [Draw.MolToImage(mol) for mol in mols]
for i, image in enumerate(images):
    image.save(f'mol_{i}.png')  # Save the molecular images

# Generate feature matrix
feature_matrix = []
for mol in mols:
    # Calculate molecular descriptors
    descriptors = [Descriptors.MolLogP(mol),
                   Descriptors.MolMR(mol),
                   Descriptors.TPSA(mol),
                   Descriptors.NumHDonors(mol),
                   Descriptors.NumHAcceptors(mol)]

    # Calculate molecular fingerprint (Morgan fingerprint)
    fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2043)

    # Convert fingerprint to a numpy array
    fingerprint_array = []
    for i in range(fingerprint.GetNumBits()):
        bit = fingerprint.GetBit(i)
        fingerprint_array.append(bit)

    # Combine descriptors and fingerprint into a feature vector
    feature_vector = descriptors + fingerprint_array
    feature_matrix.append(feature_vector)

# Convert the feature matrix to a pandas DataFrame
columns = ['LogP', 'MR', 'TPSA', 'NumHDonors', 'NumHAcceptors'] + \
          [f'Bit_{i}' for i in range(2043)]
feature_df = pd.DataFrame(feature_matrix, columns=columns)

# Save the feature matrix to a CSV file
feature_df.to_csv('feature_matrix_train.csv', index=False)

import pandas as pd
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem

# Read the SMILES from the cleaned_train.csv file
data = pd.read_csv('cleaned_test.csv')
smiles = data['SMILES']

# Generate molecular image representation
mols = [Chem.MolFromSmiles(smile) for smile in smiles]
images = [Draw.MolToImage(mol) for mol in mols]
for i, image in enumerate(images):
    image.save(f'mol_{i}.png')  # Save the molecular images

# Generate feature matrix
feature_matrix = []
for mol in mols:
    # Calculate molecular descriptors
    descriptors = [Descriptors.MolLogP(mol),
                   Descriptors.MolMR(mol),
                   Descriptors.TPSA(mol),
                   Descriptors.NumHDonors(mol),
                   Descriptors.NumHAcceptors(mol)]

    # Calculate molecular fingerprint (Morgan fingerprint)
    fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2043)

    # Convert fingerprint to a numpy array
    fingerprint_array = []
    for i in range(fingerprint.GetNumBits()):
        bit = fingerprint.GetBit(i)
        fingerprint_array.append(bit)

    # Combine descriptors and fingerprint into a feature vector
    feature_vector = descriptors + fingerprint_array
    feature_matrix.append(feature_vector)

# Convert the feature matrix to a pandas DataFrame
columns = ['LogP', 'MR', 'TPSA', 'NumHDonors', 'NumHAcceptors'] + \
          [f'Bit_{i}' for i in range(2043)]
feature_df = pd.DataFrame(feature_matrix, columns=columns)

# Save the feature matrix to a CSV file
feature_df.to_csv('feature_matrix_test.csv', index=False)

print(feature_df.shape)

"""3.1 MODEL CNN"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features, 1)

# Reshape the feature matrix to have a channel dimension
feature_matrix_train = np.reshape(feature_matrix_train, (feature_matrix_train.shape[0], num_features, 1))
feature_matrix_test = np.reshape(feature_matrix_test, (feature_matrix_test.shape[0], num_features, 1))

# Define the CNN model architecture
model = Sequential()
model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(64, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=2000)

# Make predictions on the test set
predictions = (model.predict(feature_matrix_test) > 0.5).astype(int)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features, 1)

# Reshape the feature matrix to have a channel dimension
feature_matrix_train = np.reshape(feature_matrix_train, (feature_matrix_train.shape[0], num_features, 1))
feature_matrix_test = np.reshape(feature_matrix_test, (feature_matrix_test.shape[0], num_features, 1))

# Define the CNN model architecture
model = Sequential()
model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(64, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model with early stopping
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=2000,
          validation_data=(feature_matrix_test, labels_test),
          callbacks=[early_stopping])

# Make predictions on the test set
predictions = (model.predict(feature_matrix_test) > 0.5).astype(int)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

# Calculate AUROC
auroc = roc_auc_score(labels_test, predictions)
print("AUROC:", auroc)

# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)

# Calculate specificity (True Negative Rate)
tn, fp, fn, tp = confusion_matrix(labels_test, predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Generate classification report
report = classification_report(labels_test, predictions)
print("Classification Report:")
print(report)

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, confusion_matrix

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features, 1)

# Reshape the feature matrix to have a channel dimension
feature_matrix_train = np.reshape(feature_matrix_train, (feature_matrix_train.shape[0], num_features, 1))
feature_matrix_test = np.reshape(feature_matrix_test, (feature_matrix_test.shape[0], num_features, 1))

# Define the CNN model architecture
model = Sequential()
model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(64, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=50)

# Make predictions on the test set
predictions = (model.predict(feature_matrix_test) > 0.5).astype(int)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

# Calculate AUROC
auroc = roc_auc_score(labels_test, predictions)
print("AUROC:", auroc)

# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)

# Calculate specificity (True Negative Rate)
tn, fp, fn, tp = confusion_matrix(labels_test, predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Generate classification report
report = classification_report(labels_test, predictions)
print("Classification Report:")
print(report)

# Save the model weights
model.save_weights('model_weights.h5')

"""3.2 GraphSAGE"""

!pip install networkx
!pip install igraph
!pip install spektral

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_train.npy', adjacency_matrix_train)

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_test.npy', adjacency_matrix_train)

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Reshape
from spektral.layers import GraphSageConv
from spektral.datasets import citation
from spektral.utils import normalized_adjacency
from scipy.sparse import coo_matrix
from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the adjacency matrix
adjacency_matrix_train = np.load('adjacency_matrix_train.npy')
adjacency_matrix_test = np.load('adjacency_matrix_test.npy')

# Convert adjacency matrix to SparseTensor
adjacency_matrix_train = coo_matrix(adjacency_matrix_train)
indices = np.column_stack((adjacency_matrix_train.row, adjacency_matrix_train.col))
adjacency_matrix_train = tf.sparse.SparseTensor(indices, adjacency_matrix_train.data, adjacency_matrix_train.shape)

adjacency_matrix_test = coo_matrix(adjacency_matrix_test)
indices = np.column_stack((adjacency_matrix_test.row, adjacency_matrix_test.col))
adjacency_matrix_test = tf.sparse.SparseTensor(indices, adjacency_matrix_test.data, adjacency_matrix_test.shape)

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features,)

# Define the GraphSAGE model architecture
x_in = tf.keras.Input(shape=input_shape)
a_in = tf.keras.Input(shape=(None,), sparse=True)

x = GraphSageConv(64, activation='relu')([x_in, a_in])
x = Dropout(0.5)(x)
x = GraphSageConv(64, activation='relu')([x, a_in])
x = Dropout(0.5)(x)
x = tf.keras.layers.Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(units=1, activation='sigmoid')(x)

# Create the model
model = Model(inputs=[x_in, a_in], outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x=[feature_matrix_train, adjacency_matrix_train], y=labels_train, batch_size=32, epochs=2000, validation_data=([feature_matrix_test, adjacency_matrix_test], labels_test))

# Evaluate the model
y_pred = model.predict([feature_matrix_test, adjacency_matrix_test])
y_pred_binary = (y_pred >= 0.5).astype(int)
print("Classification Report:")
print(classification_report(labels_test, y_pred_binary))

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(labels_test, y_pred)
aupr = np.trapz(precision, recall)
#print(f"AUPR: {aupr}")

# Calculate AUROC
auroc = roc_auc_score(labels_test, y_pred)
print(f"AUROC: {auroc}")

# Calculate specificity

"""GRAPH METHODS

1.GAT
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GAT model
class GAT(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1):
        super(GAT, self).__init__()
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, output_dim, heads=1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GAT model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2

# Create an instance of the GAT model
model = GAT(input_dim, hidden_dim, output_dim, num_heads)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""2.GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""3.GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv

# Load the feature matrix from the file
feature_matrix_test = np.load('feature_matrix_test.npy')

# Define the adjacency matrix (graph structure)
adjacency_matrix = np.eye(feature_matrix_test.shape[0])  # Assuming an identity adjacency matrix

# Convert the feature matrix and adjacency matrix to PyTorch tensors
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
adjacency_matrix = torch.from_numpy(adjacency_matrix).float()

# Create the edge_index tensor
num_nodes = feature_matrix_test.shape[0]
edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()

# Create a PyTorch Geometric data object
data = Data(x=feature_matrix_test, edge_index=edge_index, edge_attr=adjacency_matrix)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_test.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Perform a forward pass to obtain the node embeddings
node_embeddings = model(data.x, data.edge_index)

# Print the shape of the node embeddings
#print(f"Node embeddings shape: {node_embeddings.shape}")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""ML methods

3.1 Decision Tree
"""

import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Create a Decision Tree classifier
dt_classifier = DecisionTreeClassifier(random_state=42)

# Train the classifier
dt_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = dt_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Create a Decision Tree classifier as the base estimator
base_estimator = DecisionTreeClassifier()

# Create a Bagging classifier
bagging_classifier = BaggingClassifier(base_estimator=base_estimator)

# Train the classifier
bagging_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = bagging_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

"""3.2 Random Forest"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Create a Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier
rf_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = rf_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

"""Bagging"""

import pandas as pd
import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Create a base estimator (e.g., Decision Tree)
base_estimator = DecisionTreeClassifier()

# Create a Bagging classifier
bagging_classifier = BaggingClassifier(base_estimator=base_estimator, n_estimators=100, random_state=42)

# Train the classifier
bagging_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = bagging_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

"""Boosting"""

import pandas as pd
import numpy as np
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Create a base estimator (e.g., Decision Tree)
base_estimator = DecisionTreeClassifier()

# Create an AdaBoost classifier
boosting_classifier = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=100, random_state=42)

# Train the classifier
boosting_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = boosting_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

import pandas as pd
import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.tree import DecisionTreeClassifier

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Create a base estimator (e.g., Decision Tree)
base_estimator = DecisionTreeClassifier()

# Create a Bagging classifier
bagging_classifier = BaggingClassifier(base_estimator=base_estimator, n_estimators=100, random_state=42)

# Train the classifier
bagging_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = bagging_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

# Calculate precision
precision = precision_score(labels_test, predictions)
print("Precision:", precision)

# Calculate recall
recall = recall_score(labels_test, predictions)
print("Recall:", recall)

# Calculate F1-score
f1 = f1_score(labels_test, predictions)
print("F1-score:", f1)

import pandas as pd
import numpy as np
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.tree import DecisionTreeClassifier

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Create a base estimator (e.g., Decision Tree)
base_estimator = DecisionTreeClassifier()

# Create an AdaBoost classifier
boosting_classifier = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=100, random_state=42)

# Train the classifier
boosting_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = boosting_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

# Calculate precision
precision = precision_score(labels_test, predictions)
print("Precision:", precision)

# Calculate recall
recall = recall_score(labels_test, predictions)
print("Recall:", recall)

# Calculate F1-score
f1 = f1_score(labels_test, predictions)
print("F1-score:", f1)

"""3.3 XGBoost"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import accuracy_score

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Initialize the XGBoost classifier
xgb_classifier = xgb.XGBClassifier()

# Train the classifier
xgb_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = xgb_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

"""# COMBINED DESCRIPTORS AND FEATURIZATION - TWO LEVEL AND MULTILEVEL GRANULARITY

4. TWO LEVEL GRANULARITY - AC+SS
"""

#training data
import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Atomic representation
atomic_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    atomic_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)  # Modify parameters as needed
    atomic_features_train.append(atomic_features)

atomic_features_train = np.array(atomic_features_train)

# Substring representation (using n-grams)
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_train = np.array(substring_features_train)

# Generate feature matrix
feature_matrix_train = np.concatenate((atomic_features_train, substring_features_train), axis=1)

# Print the shape of the feature matrix
print("Feature matrix shape:", feature_matrix_train.shape)

# Save the substring features to a .npy file
np.save('features_matrix_train.npy', feature_matrix_train)

#testing data
import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Atomic representation
atomic_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    atomic_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)  # Modify parameters as needed
    atomic_features_train.append(atomic_features)

atomic_features_train = np.array(atomic_features_train)

# Substring representation (using n-grams)
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_train = np.array(substring_features_train)

# Generate feature matrix
feature_matrix_train = np.concatenate((atomic_features_train, substring_features_train), axis=1)

# Print the shape of the feature matrix
print("Feature matrix shape:", feature_matrix_train.shape)

# Save the substring features to a .npy file
np.save('features_matrix_test.npy', feature_matrix_train)

"""4.1. Bi-RNN"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, SimpleRNN, Dropout, Reshape

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the Bi-RNN model architecture
model = Sequential()
model.add(Bidirectional(SimpleRNN(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=100, validation_data=(feature_matrix_test, labels_test))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert the predicted probabilities to binary predictions
binary_predictions = (predictions > 0.5).astype(int)

# Compute accuracy
accuracy = accuracy_score(labels_test, binary_predictions)
print(f"Accuracy: {accuracy}")

# Compute precision
precision = precision_score(labels_test, binary_predictions)
print(f"Precision: {precision}")

# Compute recall
recall = recall_score(labels_test, binary_predictions)
print(f"Recall: {recall}")

# Compute F1 score
f1 = f1_score(labels_test, binary_predictions)
print(f"F1 Score: {f1}")

from sklearn.metrics import roc_auc_score

# Calculate AUROC
auroc = roc_auc_score(labels_test, predictions)
print(f"AUROC: {auroc}")

from sklearn.metrics import average_precision_score

# Assuming we have the true labels and predicted probabilities
true_labels = labels_test
predicted_probs = predictions

# Calculate the average precision score (AUPR)
aupr = average_precision_score(true_labels, predicted_probs)

print(f"AUPR: {aupr}")
from sklearn.metrics import confusion_matrix
# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Early Stopping Bi-RNN"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, SimpleRNN, Dropout, Reshape
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the Bi-RNN model architecture
model = Sequential()
model.add(Bidirectional(SimpleRNN(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=2000, validation_data=(feature_matrix_test, labels_test), callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Save the model weights
model.save_weights('model_weights.h5')

"""4.2 Bi-LSTM"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Dropout, Reshape

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the Bi-LSTM model architecture
model = Sequential()
model.add(Bidirectional(LSTM(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=100, validation_data=(feature_matrix_test, labels_test))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert the predicted probabilities to binary predictions
binary_predictions = (predictions > 0.5).astype(int)

# Compute accuracy
accuracy = accuracy_score(labels_test, binary_predictions)
print(f"Accuracy: {accuracy}")

# Compute precision
precision = precision_score(labels_test, binary_predictions)
print(f"Precision: {precision}")

# Compute recall
recall = recall_score(labels_test, binary_predictions)
print(f"Recall: {recall}")

# Compute F1 score
f1 = f1_score(labels_test, binary_predictions)
print(f"F1 Score: {f1}")

# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Early Stopping Bi-LSTM"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Dropout, Reshape
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the Bi-LSTM model architecture
model = Sequential()
model.add(Bidirectional(LSTM(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=2000, validation_data=(feature_matrix_test, labels_test), callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Save the model weights
model.save_weights('model_weights.h5')

"""4.3 Bi-GRU"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, GRU, Dropout, Reshape

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the bi-GRU model architecture
model = Sequential()
model.add(Bidirectional(GRU(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=50, validation_data=(feature_matrix_test, labels_test))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert the predicted probabilities to binary predictions
binary_predictions = (predictions > 0.5).astype(int)

# Compute accuracy
accuracy = accuracy_score(labels_test, binary_predictions)
print(f"Accuracy: {accuracy}")

# Compute precision
precision = precision_score(labels_test, binary_predictions)
print(f"Precision: {precision}")

# Compute recall
recall = recall_score(labels_test, binary_predictions)
print(f"Recall: {recall}")

# Compute F1 score
f1 = f1_score(labels_test, binary_predictions)
print(f"F1 Score: {f1}")

# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)
from sklearn.metrics import confusion_matrix

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Early Stopping Bi-GRU"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, GRU, Dropout, Reshape
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the bi-GRU model architecture
model = Sequential()
model.add(Bidirectional(GRU(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=2000, validation_data=(feature_matrix_test, labels_test), callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Save the model weights
model.save_weights('model_weights.h5')

"""4.4 GraphSAGE"""

!pip install networkx
!pip install igraph
!pip install spektral

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_train.npy', adjacency_matrix_train)

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_test.npy', adjacency_matrix_train)

"""EARLY STOPPING GRAPHSAGE"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Reshape
from spektral.layers import GraphSageConv
from spektral.datasets import citation
from spektral.utils import normalized_adjacency
from scipy.sparse import coo_matrix
from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the adjacency matrix
adjacency_matrix_train = np.load('adjacency_matrix_train.npy')
adjacency_matrix_test = np.load('adjacency_matrix_test.npy')

# Convert adjacency matrix to SparseTensor
adjacency_matrix_train = coo_matrix(adjacency_matrix_train)
indices = np.column_stack((adjacency_matrix_train.row, adjacency_matrix_train.col))
adjacency_matrix_train = tf.sparse.SparseTensor(indices, adjacency_matrix_train.data, adjacency_matrix_train.shape)

adjacency_matrix_test = coo_matrix(adjacency_matrix_test)
indices = np.column_stack((adjacency_matrix_test.row, adjacency_matrix_test.col))
adjacency_matrix_test = tf.sparse.SparseTensor(indices, adjacency_matrix_test.data, adjacency_matrix_test.shape)

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features,)

# Define the GraphSAGE model architecture
x_in = tf.keras.Input(shape=input_shape)
a_in = tf.keras.Input(shape=(None,), sparse=True)

x = GraphSageConv(64, activation='relu')([x_in, a_in])
x = Dropout(0.5)(x)
x = GraphSageConv(64, activation='relu')([x, a_in])
x = Dropout(0.5)(x)
x = tf.keras.layers.Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(units=1, activation='sigmoid')(x)

# Create the model
model = Model(inputs=[x_in, a_in], outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(x=[feature_matrix_train, adjacency_matrix_train], y=labels_train, batch_size=32, epochs=2000,
          validation_data=([feature_matrix_test, adjacency_matrix_test], labels_test),
          callbacks=[early_stopping])

# Evaluate the model
y_pred = model.predict([feature_matrix_test, adjacency_matrix_test])
y_pred_binary = (y_pred >= 0.5).astype(int)
print("Classification Report:")
print(classification_report(labels_test, y_pred_binary))

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(labels_test, y_pred)
aupr = np.trapz(precision, recall)
#print(f"AUPR: {aupr}")

# Calculate AUROC
auroc = roc_auc_score(labels_test, y_pred)
print(f"AUROC: {auroc}")

"""4.5 GAT"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GAT model
class GAT(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1):
        super(GAT, self).__init__()
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, output_dim, heads=1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GAT model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2

# Create an instance of the GAT model
model = GAT(input_dim, hidden_dim, output_dim, num_heads)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv

# Load the feature matrix from the file
feature_matrix_test = np.load('feature_matrix_test.npy')

# Define the adjacency matrix (graph structure)
adjacency_matrix = np.eye(feature_matrix_test.shape[0])  # Assuming an identity adjacency matrix

# Convert the feature matrix and adjacency matrix to PyTorch tensors
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
adjacency_matrix = torch.from_numpy(adjacency_matrix).float()

# Create the edge_index tensor
num_nodes = feature_matrix_test.shape[0]
edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()

# Create a PyTorch Geometric data object
data = Data(x=feature_matrix_test, edge_index=edge_index, edge_attr=adjacency_matrix)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_test.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Perform a forward pass to obtain the node embeddings
node_embeddings = model(data.x, data.edge_index)

# Print the shape of the node embeddings
#print(f"Node embeddings shape: {node_embeddings.shape}")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""ML BASED METHODS

1. RF
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Create a Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier
rf_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = rf_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

# Generate classification report
classification_rep = classification_report(labels_test, predictions)
print("Classification Report:\n", classification_rep)

# Calculate AUROC
probabilities = rf_classifier.predict_proba(feature_matrix_test)[:, 1]
auroc = roc_auc_score(labels_test, probabilities)
print("AUROC:", auroc)

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Ensemble Methods"""

import pandas as pd
import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Create a base estimator (e.g., Decision Tree)
base_estimator = DecisionTreeClassifier()

# Create a Bagging classifier
bagging_classifier = BaggingClassifier(base_estimator=base_estimator, n_estimators=100, random_state=42)

# Train the classifier
bagging_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = bagging_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

import pandas as pd
import numpy as np
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Create a base estimator (e.g., Decision Tree)
base_estimator = DecisionTreeClassifier()

# Create an AdaBoost classifier
boosting_classifier = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=100, random_state=42)

# Train the classifier
boosting_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = boosting_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

import pandas as pd
import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.tree import DecisionTreeClassifier

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Create a base estimator (e.g., Decision Tree)
base_estimator = DecisionTreeClassifier()

# Create a Bagging classifier
bagging_classifier = BaggingClassifier(base_estimator=base_estimator, n_estimators=100, random_state=42)

# Train the classifier
bagging_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = bagging_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

# Calculate precision
precision = precision_score(labels_test, predictions)
print("Precision:", precision)

# Calculate recall
recall = recall_score(labels_test, predictions)
print("Recall:", recall)

# Calculate F1-score
f1 = f1_score(labels_test, predictions)
print("F1-score:", f1)

# Generate classification report
classification_rep = classification_report(labels_test, predictions)
print("Classification Report:\n", classification_rep)

# Calculate AUROC
probabilities = xgb_classifier.predict_proba(feature_matrix_test)[:, 1]
auroc = roc_auc_score(labels_test, probabilities)
print("AUROC:", auroc)

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""2. XGBoost"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Initialize the XGBoost classifier
xgb_classifier = xgb.XGBClassifier()

# Train the classifier
xgb_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = xgb_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

# Generate classification report
classification_rep = classification_report(labels_test, predictions)
print("Classification Report:\n", classification_rep)

# Calculate AUROC
probabilities = xgb_classifier.predict_proba(feature_matrix_test)[:, 1]
auroc = roc_auc_score(labels_test, probabilities)
print("AUROC:", auroc)

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""5. TWO LEVEL GRANULARITY - SS+IMG"""

import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Molecular representation
mol_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048)  # Modify parameters as needed
    mol_features_train.append(mol_features)

mol_features_train = np.array(mol_features_train)

# Print the shape of the molecular features
print("Molecular features shape:", mol_features_train.shape)

# Substring representation
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_train = np.array(substring_features_train)

# Print the shape of the substring features
print("Substring features shape:", substring_features_train.shape)

# Save the feature matrices
np.save('feature_matrix_train_molecular.npy', mol_features_train)
np.save('feature_matrix_train_substring.npy', substring_features_train)
import numpy as np

# Load the molecular and substring feature matrices
mol_features_train = np.load('feature_matrix_train_molecular.npy')
substring_features_train = np.load('feature_matrix_train_substring.npy')

# Concatenate the features
combined_features_train = np.concatenate((mol_features_train, substring_features_train), axis=1)

# Print the shape of the combined feature matrix
print("Combined features shape:", combined_features_train.shape)

# Save the combined feature matrix
np.save('feature_matrix_train.npy', combined_features_train)

import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Molecular representation
mol_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048)  # Modify parameters as needed
    mol_features_train.append(mol_features)

mol_features_train = np.array(mol_features_train)

# Print the shape of the molecular features
print("Molecular features shape:", mol_features_train.shape)

# Substring representation
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_train = np.array(substring_features_train)

# Print the shape of the substring features
print("Substring features shape:", substring_features_train.shape)

# Save the feature matrices
np.save('feature_matrix_train_molecular.npy', mol_features_train)
np.save('feature_matrix_train_substring.npy', substring_features_train)
import numpy as np

# Load the molecular and substring feature matrices
mol_features_train = np.load('feature_matrix_train_molecular.npy')
substring_features_train = np.load('feature_matrix_train_substring.npy')

# Concatenate the features
combined_features_train = np.concatenate((mol_features_train, substring_features_train), axis=1)

# Print the shape of the combined feature matrix
print("Combined features shape:", combined_features_train.shape)

# Save the combined feature matrix
np.save('feature_matrix_test.npy', combined_features_train)

"""5.1. Bi-RNN"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, SimpleRNN, Dropout, Reshape

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the Bi-RNN model architecture
model = Sequential()
model.add(Bidirectional(SimpleRNN(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=100, validation_data=(feature_matrix_test, labels_test))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert the predicted probabilities to binary predictions
binary_predictions = (predictions > 0.5).astype(int)

# Compute accuracy
accuracy = accuracy_score(labels_test, binary_predictions)
print(f"Accuracy: {accuracy}")

# Compute precision
precision = precision_score(labels_test, binary_predictions)
print(f"Precision: {precision}")

# Compute recall
recall = recall_score(labels_test, binary_predictions)
print(f"Recall: {recall}")

# Compute F1 score
f1 = f1_score(labels_test, binary_predictions)
print(f"F1 Score: {f1}")

from sklearn.metrics import average_precision_score

# Assuming we have the true labels and predicted probabilities
true_labels = labels_test
predicted_probs = predictions

# Calculate the average precision score (AUPR)
aupr = average_precision_score(true_labels, predicted_probs)

print(f"AUPR: {aupr}")
from sklearn.metrics import confusion_matrix
# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Early Stopping Bi-RNN"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, SimpleRNN, Dropout, Reshape
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the Bi-RNN model architecture
model = Sequential()
model.add(Bidirectional(SimpleRNN(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=2000, validation_data=(feature_matrix_test, labels_test), callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Save the model weights
model.save_weights('model_weights.h5')

"""5.2. Bi-LSTM"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Dropout, Reshape

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the Bi-LSTM model architecture
model = Sequential()
model.add(Bidirectional(LSTM(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=100, validation_data=(feature_matrix_test, labels_test))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert the predicted probabilities to binary predictions
binary_predictions = (predictions > 0.5).astype(int)

# Compute accuracy
accuracy = accuracy_score(labels_test, binary_predictions)
print(f"Accuracy: {accuracy}")

# Compute precision
precision = precision_score(labels_test, binary_predictions)
print(f"Precision: {precision}")

# Compute recall
recall = recall_score(labels_test, binary_predictions)
print(f"Recall: {recall}")

# Compute F1 score
f1 = f1_score(labels_test, binary_predictions)
print(f"F1 Score: {f1}")

from sklearn.metrics import average_precision_score

# Assuming we have the true labels and predicted probabilities
true_labels = labels_test
predicted_probs = predictions

# Calculate the average precision score (AUPR)
aupr = average_precision_score(true_labels, predicted_probs)

print(f"AUPR: {aupr}")
from sklearn.metrics import confusion_matrix
# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Early Stopping Bi-LSTM"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Dropout, Reshape
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the Bi-LSTM model architecture
model = Sequential()
model.add(Bidirectional(LSTM(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=2000, validation_data=(feature_matrix_test, labels_test), callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Save the model weights
model.save_weights('model_weights.h5')

"""5.3 Bi-GRU"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, GRU, Dropout, Reshape

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the bi-GRU model architecture
model = Sequential()
model.add(Bidirectional(GRU(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=50, validation_data=(feature_matrix_test, labels_test))

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert the predicted probabilities to binary predictions
binary_predictions = (predictions > 0.5).astype(int)

# Compute accuracy
accuracy = accuracy_score(labels_test, binary_predictions)
print(f"Accuracy: {accuracy}")

# Compute precision
precision = precision_score(labels_test, binary_predictions)
print(f"Precision: {precision}")

# Compute recall
recall = recall_score(labels_test, binary_predictions)
print(f"Recall: {recall}")

# Compute F1 score
f1 = f1_score(labels_test, binary_predictions)
print(f"F1 Score: {f1}")

from sklearn.metrics import average_precision_score

# Assuming we have the true labels and predicted probabilities
true_labels = labels_test
predicted_probs = predictions

# Calculate the average precision score (AUPR)
aupr = average_precision_score(true_labels, predicted_probs)

print(f"AUPR: {aupr}")
from sklearn.metrics import confusion_matrix
# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, binary_predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Early Stopping Bi-GRU"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Bidirectional, GRU, Dropout, Reshape
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Reshape the feature matrix to include a sequence length dimension
sequence_length_train = feature_matrix_train.shape[0]
feature_matrix_train = np.reshape(feature_matrix_train, (sequence_length_train, 1, -1))

sequence_length_test = feature_matrix_test.shape[0]
feature_matrix_test = np.reshape(feature_matrix_test, (sequence_length_test, 1, -1))

# Set the input shape
input_shape = (1, feature_matrix_train.shape[2])

# Define the bi-GRU model architecture
model = Sequential()
model.add(Bidirectional(GRU(64, activation='relu'), input_shape=input_shape))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(feature_matrix_train, labels_train, batch_size=32, epochs=2000, validation_data=(feature_matrix_test, labels_test), callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(feature_matrix_test, labels_test)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

# Make predictions
predictions = model.predict(feature_matrix_test)

# Save the model weights
model.save_weights('model_weights.h5')

"""5.4 GraphSAGE"""

!pip install networkx
!pip install igraph
!pip install spektral

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_train.npy', adjacency_matrix_train)

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_test.npy', adjacency_matrix_train)

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Reshape
from spektral.layers import GraphSageConv
from spektral.datasets import citation
from spektral.utils import normalized_adjacency
from scipy.sparse import coo_matrix
from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the adjacency matrix
adjacency_matrix_train = np.load('adjacency_matrix_train.npy')
adjacency_matrix_test = np.load('adjacency_matrix_test.npy')

# Convert adjacency matrix to SparseTensor
adjacency_matrix_train = coo_matrix(adjacency_matrix_train)
indices = np.column_stack((adjacency_matrix_train.row, adjacency_matrix_train.col))
adjacency_matrix_train = tf.sparse.SparseTensor(indices, adjacency_matrix_train.data, adjacency_matrix_train.shape)

adjacency_matrix_test = coo_matrix(adjacency_matrix_test)
indices = np.column_stack((adjacency_matrix_test.row, adjacency_matrix_test.col))
adjacency_matrix_test = tf.sparse.SparseTensor(indices, adjacency_matrix_test.data, adjacency_matrix_test.shape)

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features,)

# Define the GraphSAGE model architecture
x_in = tf.keras.Input(shape=input_shape)
a_in = tf.keras.Input(shape=(None,), sparse=True)

x = GraphSageConv(64, activation='relu')([x_in, a_in])
x = Dropout(0.5)(x)
x = GraphSageConv(64, activation='relu')([x, a_in])
x = Dropout(0.5)(x)
x = tf.keras.layers.Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(units=1, activation='sigmoid')(x)

# Create the model
model = Model(inputs=[x_in, a_in], outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x=[feature_matrix_train, adjacency_matrix_train], y=labels_train, batch_size=32, epochs=2000, validation_data=([feature_matrix_test, adjacency_matrix_test], labels_test))

# Evaluate the model
y_pred = model.predict([feature_matrix_test, adjacency_matrix_test])
y_pred_binary = (y_pred >= 0.5).astype(int)
print("Classification Report:")
print(classification_report(labels_test, y_pred_binary))

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(labels_test, y_pred)
aupr = np.trapz(precision, recall)
#print(f"AUPR: {aupr}")

# Calculate AUROC
auroc = roc_auc_score(labels_test, y_pred)
print(f"AUROC: {auroc}")

"""EARLY STOPPING GRAPHSAGE"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Reshape
from spektral.layers import GraphSageConv
from spektral.datasets import citation
from spektral.utils import normalized_adjacency
from scipy.sparse import coo_matrix
from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the adjacency matrix
adjacency_matrix_train = np.load('adjacency_matrix_train.npy')
adjacency_matrix_test = np.load('adjacency_matrix_test.npy')

# Convert adjacency matrix to SparseTensor
adjacency_matrix_train = coo_matrix(adjacency_matrix_train)
indices = np.column_stack((adjacency_matrix_train.row, adjacency_matrix_train.col))
adjacency_matrix_train = tf.sparse.SparseTensor(indices, adjacency_matrix_train.data, adjacency_matrix_train.shape)

adjacency_matrix_test = coo_matrix(adjacency_matrix_test)
indices = np.column_stack((adjacency_matrix_test.row, adjacency_matrix_test.col))
adjacency_matrix_test = tf.sparse.SparseTensor(indices, adjacency_matrix_test.data, adjacency_matrix_test.shape)

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features,)

# Define the GraphSAGE model architecture
x_in = tf.keras.Input(shape=input_shape)
a_in = tf.keras.Input(shape=(None,), sparse=True)

x = GraphSageConv(64, activation='relu')([x_in, a_in])
x = Dropout(0.5)(x)
x = GraphSageConv(64, activation='relu')([x, a_in])
x = Dropout(0.5)(x)
x = tf.keras.layers.Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(units=1, activation='sigmoid')(x)

# Create the model
model = Model(inputs=[x_in, a_in], outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)

# Train the model with early stopping
model.fit(x=[feature_matrix_train, adjacency_matrix_train], y=labels_train, batch_size=32, epochs=2000,
          validation_data=([feature_matrix_test, adjacency_matrix_test], labels_test),
          callbacks=[early_stopping])

# Evaluate the model
y_pred = model.predict([feature_matrix_test, adjacency_matrix_test])
y_pred_binary = (y_pred >= 0.5).astype(int)
print("Classification Report:")
print(classification_report(labels_test, y_pred_binary))

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(labels_test, y_pred)
aupr = np.trapz(precision, recall)
#print(f"AUPR: {aupr}")

# Calculate AUROC
auroc = roc_auc_score(labels_test, y_pred)
print(f"AUROC: {auroc}")

"""GAT"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GAT model
class GAT(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1):
        super(GAT, self).__init__()
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, output_dim, heads=1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GAT model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2

# Create an instance of the GAT model
model = GAT(input_dim, hidden_dim, output_dim, num_heads)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv

# Load the feature matrix from the file
feature_matrix_test = np.load('feature_matrix_test.npy')

# Define the adjacency matrix (graph structure)
adjacency_matrix = np.eye(feature_matrix_test.shape[0])  # Assuming an identity adjacency matrix

# Convert the feature matrix and adjacency matrix to PyTorch tensors
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
adjacency_matrix = torch.from_numpy(adjacency_matrix).float()

# Create the edge_index tensor
num_nodes = feature_matrix_test.shape[0]
edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()

# Create a PyTorch Geometric data object
data = Data(x=feature_matrix_test, edge_index=edge_index, edge_attr=adjacency_matrix)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_test.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Perform a forward pass to obtain the node embeddings
node_embeddings = model(data.x, data.edge_index)

# Print the shape of the node embeddings
#print(f"Node embeddings shape: {node_embeddings.shape}")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""ML BASED METHODS

1. RF
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Create a Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier
rf_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = rf_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

# Generate classification report
classification_rep = classification_report(labels_test, predictions)
print("Classification Report:\n", classification_rep)

# Calculate AUROC
probabilities = rf_classifier.predict_proba(feature_matrix_test)[:, 1]
auroc = roc_auc_score(labels_test, probabilities)
print("AUROC:", auroc)

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Ensemble Methods"""

import pandas as pd
import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Create a base estimator (e.g., Decision Tree)
base_estimator = DecisionTreeClassifier()

# Create a Bagging classifier
bagging_classifier = BaggingClassifier(base_estimator=base_estimator, n_estimators=100, random_state=42)

# Train the classifier
bagging_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = bagging_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

import pandas as pd
import numpy as np
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Create a base estimator (e.g., Decision Tree)
base_estimator = DecisionTreeClassifier()

# Create an AdaBoost classifier
boosting_classifier = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=100, random_state=42)

# Train the classifier
boosting_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = boosting_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

import pandas as pd
import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.tree import DecisionTreeClassifier

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Create a base estimator (e.g., Decision Tree)
base_estimator = DecisionTreeClassifier()

# Create a Bagging classifier
bagging_classifier = BaggingClassifier(base_estimator=base_estimator, n_estimators=100, random_state=42)

# Train the classifier
bagging_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = bagging_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

# Calculate precision
precision = precision_score(labels_test, predictions)
print("Precision:", precision)

# Calculate recall
recall = recall_score(labels_test, predictions)
print("Recall:", recall)

# Calculate F1-score
f1 = f1_score(labels_test, predictions)
print("F1-score:", f1)

# Generate classification report
classification_rep = classification_report(labels_test, predictions)
print("Classification Report:\n", classification_rep)

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""XGBoost"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the feature matrix for the test set
feature_matrix_test = np.load('feature_matrix_test.npy')

# Initialize the XGBoost classifier
xgb_classifier = xgb.XGBClassifier()

# Train the classifier
xgb_classifier.fit(feature_matrix_train, labels_train)

# Predict labels for the test set
predictions = xgb_classifier.predict(feature_matrix_test)

# Calculate accuracy
accuracy = accuracy_score(labels_test, predictions)
print("Accuracy:", accuracy)

# Generate classification report
classification_rep = classification_report(labels_test, predictions)
print("Classification Report:\n", classification_rep)

# Calculate AUROC
probabilities = xgb_classifier.predict_proba(feature_matrix_test)[:, 1]
auroc = roc_auc_score(labels_test, probabilities)
print("AUROC:", auroc)

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""6. THREE LEVEL GRANULARITY : FUSION FEATURES"""

import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Atomic representation
atomic_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    atomic_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)  # Modify parameters as needed
    atomic_features_train.append(atomic_features)

atomic_features_train = np.array(atomic_features_train)

# Substring representation (using n-grams)
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_train = np.array(substring_features_train)

# Molecular representation
mol_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048)  # Modify parameters as needed
    mol_features_train.append(mol_features)

mol_features_train = np.array(mol_features_train)

# Combine the feature matrices
feature_matrix_train = np.concatenate((atomic_features_train, substring_features_train, mol_features_train), axis=1)

# Print the shape of the feature matrix
print("Feature matrix shape:", feature_matrix_train.shape)

# Save the feature matrix to a .npy file
np.save('feature_matrix_train.npy', feature_matrix_train)

import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Atomic representation
atomic_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    atomic_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)  # Modify parameters as needed
    atomic_features_train.append(atomic_features)

atomic_features_train = np.array(atomic_features_train)

# Substring representation (using n-grams)
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_train = np.array(substring_features_train)

# Molecular representation
mol_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048)  # Modify parameters as needed
    mol_features_train.append(mol_features)

mol_features_train = np.array(mol_features_train)

# Combine the feature matrices
feature_matrix_train = np.concatenate((atomic_features_train, substring_features_train, mol_features_train), axis=1)

# Print the shape of the feature matrix
print("Feature matrix shape:", feature_matrix_train.shape)

# Save the feature matrix to a .npy file
np.save('feature_matrix_test.npy', feature_matrix_train)

"""6.2 Graph Methods : GraphSAGE"""

!pip install networkx
!pip install igraph
!pip install spektral

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_train.npy', adjacency_matrix_train)

import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit import DataStructs

# Define a similarity calculation function
def compute_similarity(mol_i, mol_j):
    fp_i = AllChem.GetMorganFingerprintAsBitVect(mol_i, 2, nBits=1024)
    fp_j = AllChem.GetMorganFingerprintAsBitVect(mol_j, 2, nBits=1024)
    similarity = DataStructs.TanimotoSimilarity(fp_i, fp_j)
    return similarity

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_test.csv')

# Extract the SMILES and Label columns
smiles = df_train['SMILES'].values
labels = df_train['Label'].values

# Create an empty adjacency matrix
num_molecules = len(smiles)
adjacency_matrix_train = np.zeros((num_molecules, num_molecules))

# Create the adjacency matrix
for i in range(num_molecules):
    for j in range(i+1, num_molecules):
        # Convert SMILES strings to RDKit molecules
        mol_i = Chem.MolFromSmiles(smiles[i])
        mol_j = Chem.MolFromSmiles(smiles[j])

        # Compute molecular similarity using the similarity calculation function
        similarity = compute_similarity(mol_i, mol_j)

        # Set the adjacency matrix values based on the similarity
        adjacency_matrix_train[i, j] = similarity
        adjacency_matrix_train[j, i] = similarity

# Save the adjacency matrix
np.save('adjacency_matrix_test.npy', adjacency_matrix_train)

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Reshape
from spektral.layers import GraphSageConv
from spektral.datasets import citation
from spektral.utils import normalized_adjacency
from scipy.sparse import coo_matrix
from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the adjacency matrix
adjacency_matrix_train = np.load('adjacency_matrix_train.npy')
adjacency_matrix_test = np.load('adjacency_matrix_test.npy')

# Convert adjacency matrix to SparseTensor
adjacency_matrix_train = coo_matrix(adjacency_matrix_train)
indices = np.column_stack((adjacency_matrix_train.row, adjacency_matrix_train.col))
adjacency_matrix_train = tf.sparse.SparseTensor(indices, adjacency_matrix_train.data, adjacency_matrix_train.shape)

adjacency_matrix_test = coo_matrix(adjacency_matrix_test)
indices = np.column_stack((adjacency_matrix_test.row, adjacency_matrix_test.col))
adjacency_matrix_test = tf.sparse.SparseTensor(indices, adjacency_matrix_test.data, adjacency_matrix_test.shape)

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features,)

# Define the GraphSAGE model architecture
x_in = tf.keras.Input(shape=input_shape)
a_in = tf.keras.Input(shape=(None,), sparse=True)

x = GraphSageConv(64, activation='relu')([x_in, a_in])
x = Dropout(0.5)(x)
x = GraphSageConv(64, activation='relu')([x, a_in])
x = Dropout(0.5)(x)
x = tf.keras.layers.Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(units=1, activation='sigmoid')(x)

# Create the model
model = Model(inputs=[x_in, a_in], outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x=[feature_matrix_train, adjacency_matrix_train], y=labels_train, batch_size=32, epochs=2000, validation_data=([feature_matrix_test, adjacency_matrix_test], labels_test))

# Evaluate the model
y_pred = model.predict([feature_matrix_test, adjacency_matrix_test])
y_pred_binary = (y_pred >= 0.5).astype(int)
print("Classification Report:")
print(classification_report(labels_test, y_pred_binary))

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(labels_test, y_pred)
aupr = np.trapz(precision, recall)
#print(f"AUPR: {aupr}")

# Calculate AUROC
auroc = roc_auc_score(labels_test, y_pred)
print(f"AUROC: {auroc}")

# Calculate specificity

"""EARLY STOPPING GRAPHSAGE"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Reshape
from spektral.layers import GraphSageConv
from spektral.datasets import citation
from spektral.utils import normalized_adjacency
from scipy.sparse import coo_matrix
from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score
from tensorflow.keras.callbacks import EarlyStopping

# Load the training dataset
feature_matrix_train = np.load('feature_matrix_train.npy')
df_train = pd.read_csv('cleaned_train.csv')
labels_train = df_train['Label'].values

# Load the test dataset
feature_matrix_test = np.load('feature_matrix_test.npy')
df_test = pd.read_csv('cleaned_test.csv')
labels_test = df_test['Label'].values

# Load the adjacency matrix
adjacency_matrix_train = np.load('adjacency_matrix_train.npy')
adjacency_matrix_test = np.load('adjacency_matrix_test.npy')

# Convert adjacency matrix to SparseTensor
adjacency_matrix_train = coo_matrix(adjacency_matrix_train)
indices = np.column_stack((adjacency_matrix_train.row, adjacency_matrix_train.col))
adjacency_matrix_train = tf.sparse.SparseTensor(indices, adjacency_matrix_train.data, adjacency_matrix_train.shape)

adjacency_matrix_test = coo_matrix(adjacency_matrix_test)
indices = np.column_stack((adjacency_matrix_test.row, adjacency_matrix_test.col))
adjacency_matrix_test = tf.sparse.SparseTensor(indices, adjacency_matrix_test.data, adjacency_matrix_test.shape)

# Set the input shape
num_features = feature_matrix_train.shape[1]
input_shape = (num_features,)

# Define the GraphSAGE model architecture
x_in = tf.keras.Input(shape=input_shape)
a_in = tf.keras.Input(shape=(None,), sparse=True)

x = GraphSageConv(64, activation='relu')([x_in, a_in])
x = Dropout(0.5)(x)
x = GraphSageConv(64, activation='relu')([x, a_in])
x = Dropout(0.5)(x)
x = tf.keras.layers.Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(units=1, activation='sigmoid')(x)

# Create the model
model = Model(inputs=[x_in, a_in], outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model with early stopping
model.fit(x=[feature_matrix_train, adjacency_matrix_train], y=labels_train, batch_size=32, epochs=2000,
          validation_data=([feature_matrix_test, adjacency_matrix_test], labels_test),
          callbacks=[early_stopping])

# Evaluate the model
y_pred = model.predict([feature_matrix_test, adjacency_matrix_test])
y_pred_binary = (y_pred >= 0.5).astype(int)
print("Classification Report:")
print(classification_report(labels_test, y_pred_binary))

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(labels_test, y_pred)
aupr = np.trapz(precision, recall)
#print(f"AUPR: {aupr}")

# Calculate AUROC
auroc = roc_auc_score(labels_test, y_pred)
print(f"AUROC: {auroc}")

"""GAT"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GAT model
class GAT(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1):
        super(GAT, self).__init__()
        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads)
        self.conv2 = GATConv(hidden_dim * num_heads, output_dim, heads=1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GAT model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 8
output_dim = 2
num_heads = 2

# Create an instance of the GAT model
model = GAT(input_dim, hidden_dim, output_dim, num_heads)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GIN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GINConv, global_add_pool
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GIN model
class GIN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GIN, self).__init__()
        self.conv1 = GINConv(nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        ))
        self.conv2 = GINConv(nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        ))

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GIN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GIN model
model = GIN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

# Classification Report
report = classification_report(target_labels_test, predicted_labels_test)
print("Classification Report:\n", report)

# AUROC
probs = F.softmax(test_output, dim=1)
prob_inhibitor = probs[:, 1].detach().numpy()
auroc = roc_auc_score(target_labels_test, prob_inhibitor)
print("AUROC:", auroc)

# Specificity
tn, fp, fn, tp = confusion_matrix(target_labels_test, predicted_labels_test).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

# Precision-Recall Curve and AUPR
precision, recall, _ = precision_recall_curve(target_labels_test, prob_inhibitor)
aupr = auc(recall, precision)
print("AUPR:", aupr)

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""GCN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv

# Load the feature matrix from the file
feature_matrix_test = np.load('feature_matrix_test.npy')

# Define the adjacency matrix (graph structure)
adjacency_matrix = np.eye(feature_matrix_test.shape[0])  # Assuming an identity adjacency matrix

# Convert the feature matrix and adjacency matrix to PyTorch tensors
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
adjacency_matrix = torch.from_numpy(adjacency_matrix).float()

# Create the edge_index tensor
num_nodes = feature_matrix_test.shape[0]
edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()

# Create a PyTorch Geometric data object
data = Data(x=feature_matrix_test, edge_index=edge_index, edge_attr=adjacency_matrix)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_test.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Perform a forward pass to obtain the node embeddings
node_embeddings = model(data.x, data.edge_index)

# Print the shape of the node embeddings
#print(f"Node embeddings shape: {node_embeddings.shape}")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import pandas as pd
import numpy as np

# Load the cleaned train and test data
df_train = pd.read_csv('cleaned_train.csv')
df_test = pd.read_csv('cleaned_test.csv')

# Load the feature matrices
feature_matrix_train = np.load('feature_matrix_train.npy')
feature_matrix_test = np.load('feature_matrix_test.npy')

# Load the target labels
target_labels_train = df_train['Label'].values
target_labels_test = df_test['Label'].values

# Convert the feature matrices and target labels to PyTorch tensors
feature_matrix_train = torch.from_numpy(feature_matrix_train).float()
feature_matrix_test = torch.from_numpy(feature_matrix_test).float()
target_labels_train = torch.from_numpy(target_labels_train).long()
target_labels_test = torch.from_numpy(target_labels_test).long()

# Define the adjacency matrices (graph structures) as identity matrices
adjacency_matrix_train = torch.eye(feature_matrix_train.shape[0])
adjacency_matrix_test = torch.eye(feature_matrix_test.shape[0])

# Create the edge_index tensors as self-loops
num_nodes_train = feature_matrix_train.shape[0]
edge_index_train = torch.tensor([[i, i] for i in range(num_nodes_train)], dtype=torch.long).t().contiguous()

num_nodes_test = feature_matrix_test.shape[0]
edge_index_test = torch.tensor([[i, i] for i in range(num_nodes_test)], dtype=torch.long).t().contiguous()

# Create PyTorch Geometric data objects for train and test data
data_train = Data(x=feature_matrix_train, edge_index=edge_index_train, edge_attr=adjacency_matrix_train)
data_test = Data(x=feature_matrix_test, edge_index=edge_index_test, edge_attr=adjacency_matrix_test)

# Define the GCN model
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define the dimensions of the GCN model
input_dim = feature_matrix_train.shape[1]
hidden_dim = 64
output_dim = 32

# Create an instance of the GCN model
model = GCN(input_dim, hidden_dim, output_dim)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Early stopping parameters
best_loss = float('inf')
best_model_state_dict = None
patience = 100
counter = 0

# Training loop
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    train_output = model(data_train.x, data_train.edge_index)
    # Compute the training loss
    train_loss = criterion(train_output, target_labels_train)
    # Backward pass
    train_loss.backward()
    optimizer.step()

    model.eval()
    # Forward pass on the test data
    test_output = model(data_test.x, data_test.edge_index)
    # Compute the predicted labels for the test data
    _, predicted_labels_test = torch.max(test_output, dim=1)

    # Calculate the accuracy
    correct = (predicted_labels_test == target_labels_test).sum().item()
    total = target_labels_test.size(0)
    accuracy = correct / total

    print(f"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss.item()} | Test Accuracy: {accuracy}")

    # Check if the validation loss has improved
    if train_loss < best_loss:
        best_loss = train_loss
        best_model_state_dict = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

# Load the best model state dict
model.load_state_dict(best_model_state_dict)

# Evaluation
model.eval()
# Forward pass on the test data using the best model
test_output = model(data_test.x, data_test.edge_index)
# Compute the predicted labels for the test data
_, predicted_labels_test = torch.max(test_output, dim=1)

# Calculate the accuracy
correct = (predicted_labels_test == target_labels_test).sum().item()
total = target_labels_test.size(0)
accuracy = correct / total

print(f"Test Accuracy: {accuracy}")

# Save the model weights
torch.save(model.state_dict(), 'model_weights.pt')

"""ML CLASSIFIERS

1. XGBoost
"""

import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np
import xgboost as xgb
from sklearn.metrics import average_precision_score, classification_report, confusion_matrix

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Initialize the XGBoost classifier
xgb_classifier = xgb.XGBClassifier()

# Train the classifier
xgb_classifier.fit(feature_matrix_train, labels_train)

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
smiles_test = df_test['SMILES'].values
labels_test = df_test['Label'].values

# Atomic representation
atomic_features_test = []
for smiles in smiles_test:
    mol = Chem.MolFromSmiles(smiles)
    atomic_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)  # Modify parameters as needed
    atomic_features_test.append(atomic_features)

atomic_features_test = np.array(atomic_features_test)

# Substring representation (using n-grams)
substring_features_test = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_test:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_test.append(substring_features)

substring_features_test = np.array(substring_features_test)

# Molecular representation
mol_features_test = []
for smiles in smiles_test:
    mol = Chem.MolFromSmiles(smiles)
    mol_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048)  # Modify parameters as needed
    mol_features_test.append(mol_features)

mol_features_test = np.array(mol_features_test)

# Combine the feature matrices for the test set
feature_matrix_test = np.concatenate((atomic_features_test, substring_features_test, mol_features_test), axis=1)

# Predict labels for the test set
predictions = xgb_classifier.predict(feature_matrix_test)

# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)

# Calculate classification report
classification_rep = classification_report(labels_test, predictions)
print("Classification Report:")
print(classification_rep)

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""2. RF"""

import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import average_precision_score, classification_report, confusion_matrix

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier
rf_classifier.fit(feature_matrix_train, labels_train)

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
smiles_test = df_test['SMILES'].values
labels_test = df_test['Label'].values

# Atomic representation
atomic_features_test = []
for smiles in smiles_test:
    mol = Chem.MolFromSmiles(smiles)
    atomic_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)  # Modify parameters as needed
    atomic_features_test.append(atomic_features)

atomic_features_test = np.array(atomic_features_test)

# Substring representation (using n-grams)
substring_features_test = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_test:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_test.append(substring_features)

substring_features_test = np.array(substring_features_test)

# Molecular representation
mol_features_test = []
for smiles in smiles_test:
    mol = Chem.MolFromSmiles(smiles)
    mol_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048)  # Modify parameters as needed
    mol_features_test.append(mol_features)

mol_features_test = np.array(mol_features_test)

# Combine the feature matrices for the test set
feature_matrix_test = np.concatenate((atomic_features_test, substring_features_test, mol_features_test), axis=1)

# Predict labels for the test set
predictions = rf_classifier.predict(feature_matrix_test)

# Calculate AUPR
aupr = average_precision_score(labels_test, predictions)
print("AUPR:", aupr)

# Calculate classification report
classification_rep = classification_report(labels_test, predictions)
print("Classification Report:")
print(classification_rep)

# Calculate specificity
tn, fp, fn, tp = confusion_matrix(labels_test, predictions).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

"""Ensemble Methods Used"""

import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import numpy as np
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import average_precision_score, classification_report, confusion_matrix
import xgboost as xgb

# Load the cleaned training dataset
df_train = pd.read_csv('cleaned_train.csv')
smiles_train = df_train['SMILES'].values
labels_train = df_train['Label'].values

# Load the feature matrix for the training set
feature_matrix_train = np.load('feature_matrix_train.npy')

# Atomic representation
atomic_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    atomic_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)  # Modify parameters as needed
    atomic_features_train.append(atomic_features)

atomic_features_train = np.array(atomic_features_train)

# Substring representation (using n-grams)
substring_features_train = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_train.append(substring_features)

substring_features_train = np.array(substring_features_train)

# Molecular representation
mol_features_train = []
for smiles in smiles_train:
    mol = Chem.MolFromSmiles(smiles)
    mol_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048)  # Modify parameters as needed
    mol_features_train.append(mol_features)

mol_features_train = np.array(mol_features_train)

# Combine the feature matrices for the training set
feature_matrix_train = np.concatenate((atomic_features_train, substring_features_train, mol_features_train), axis=1)

# Initialize individual classifiers
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
xgb_classifier = xgb.XGBClassifier()

# Train individual classifiers
rf_classifier.fit(feature_matrix_train, labels_train)
xgb_classifier.fit(feature_matrix_train, labels_train)

# Load the cleaned test dataset
df_test = pd.read_csv('cleaned_test.csv')
smiles_test = df_test['SMILES'].values
labels_test = df_test['Label'].values

# Atomic representation for the test set
atomic_features_test = []
for smiles in smiles_test:
    mol = Chem.MolFromSmiles(smiles)
    atomic_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)  # Modify parameters as needed
    atomic_features_test.append(atomic_features)

atomic_features_test = np.array(atomic_features_test)

# Substring representation (using n-grams) for the test set
substring_features_test = []
n_grams = 3  # Specify the desired length of n-grams

for smiles in smiles_test:
    mol = Chem.MolFromSmiles(smiles)
    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule
    substring_features = rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(mol, nBits=2048, nBitsPerEntry=2)  # Modify parameters as needed
    substring_features_test.append(substring_features)

substring_features_test = np.array(substring_features_test)

# Molecular representation for the test set
mol_features_test = []
for smiles in smiles_test:
    mol = Chem.MolFromSmiles(smiles)
    mol_features = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048)  # Modify parameters as needed
    mol_features_test.append(mol_features)

mol_features_test = np.array(mol_features_test)

# Combine the feature matrices for the test set
feature_matrix_test = np.concatenate((atomic_features_test, substring_features_test, mol_features_test), axis=1)

# Predict labels for the test set using individual classifiers
predictions_rf = rf_classifier.predict(feature_matrix_test)
predictions_xgb = xgb_classifier.predict(feature_matrix_test)

# Combine predictions using ensemble methods (Voting Classifier)
voting_classifier = VotingClassifier(estimators=[('rf', rf_classifier), ('xgb', xgb_classifier)], voting='soft')
voting_classifier.fit(feature_matrix_train, labels_train)
predictions_voting = voting_classifier.predict(feature_matrix_test)

# Calculate AUPR for individual classifiers
aupr_rf = average_precision_score(labels_test, predictions_rf)
aupr_xgb = average_precision_score(labels_test, predictions_xgb)
aupr_voting = average_precision_score(labels_test, predictions_voting)
print("AUPR (Random Forest):", aupr_rf)
print("AUPR (XGBoost):", aupr_xgb)
print("AUPR (Voting):", aupr_voting)

# Calculate classification report for individual classifiers
classification_rep_rf = classification_report(labels_test, predictions_rf)
classification_rep_xgb = classification_report(labels_test, predictions_xgb)
classification_rep_voting = classification_report(labels_test, predictions_voting)
print("Classification Report (Random Forest):")
print(classification_rep_rf)
print("Classification Report (XGBoost):")
print(classification_rep_xgb)
print("Classification Report (Voting):")
print(classification_rep_voting)

# Calculate specificity for individual classifiers
tn_rf, fp_rf, fn_rf, tp_rf = confusion_matrix(labels_test, predictions_rf).ravel()
specificity_rf = tn_rf / (tn_rf + fp_rf)

tn_xgb, fp_xgb, fn_xgb, tp_xgb = confusion_matrix(labels_test, predictions_xgb).ravel()
specificity_xgb = tn_xgb / (tn_xgb + fp_xgb)

tn_voting, fp_voting, fn_voting, tp_voting = confusion_matrix(labels_test, predictions_voting).ravel()
specificity_voting = tn_voting / (tn_voting + fp_voting)

print("Specificity (Random Forest):", specificity_rf)
print("Specificity (XGBoost):", specificity_xgb)
print("Specificity (Voting):", specificity_voting)